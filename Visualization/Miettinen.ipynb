{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miettinen et al.\n",
    "===========\n",
    "\n",
    "This notebook is used to generate the results for the evaluation of the scheme by Miettinnen et al. (Miettinen, M., Asokan, N., Nguyen, T.D., Sadeghi, A.-R., Sobhani, M.: Context-Based Zero-Interaction Pairing and Key Evolution for Advanced Personal Devices. In: Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security - CCS ’14. pp. 880–891. ACM Press, New York, New York, USA (2014).). \n",
    "\n",
    "The paper proposes two different fingerprinting approaches, one based on ambient audio ('noiseFingerprint' in the code) and one based on luminosity ('lux_miettinen'). In the paper, the scheme is evaluated in section 4.3.\n",
    "\n",
    "First, we load a couple of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "from glob import glob\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import isfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up a number of constants that allow us to find the relevant files. Change this to point to the correct paths on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAR_EXP = 'CarExp/'\n",
    "OFF_EXP = 'OfficeExp/'\n",
    "MOBILE_EXP = 'MobileExp/'\n",
    "\n",
    "BASE_DIR = \"/media/seemoo/data/zia-data/results/\"\n",
    "\n",
    "PREFIX_JSON='/home/seemoo/plots/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more internal constants. Leave these alone unless you understand what you are doing. They define the pairings of sensors for the different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor mapping: car experiment\n",
    "SENSORS_CAR1 = ['01', '02', '03', '04', '05', '06']\n",
    "SENSORS_CAR2 = ['07', '08', '09', '10', '11', '12']\n",
    "SENSORS_CAR = SENSORS_CAR1 + SENSORS_CAR2\n",
    "\n",
    "# Sensor mapping: office experiment\n",
    "SENSORS_OFFICE1 = ['01', '02', '03', '04', '05', '06', '07', '08']\n",
    "SENSORS_OFFICE2 = ['09', '10', '11', '12', '13', '14', '15', '16']\n",
    "SENSORS_OFFICE3 = ['17', '18', '19', '20', '21', '22', '23', '24']\n",
    "SENSORS_OFFICE = SENSORS_OFFICE1 + SENSORS_OFFICE2 + SENSORS_OFFICE3\n",
    "\n",
    "SENSORS_MOBILE = [\"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\"]\n",
    "SENSORS_MOBILE_LIMITED = [\"05\", \"06\", \"08\", \"09\", \"15\", \"16\", \"22\", \"23\"]\n",
    "\n",
    "COLO_CAR = np.array([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 1\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 2\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 3\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 4\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 5\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 6\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 7\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 8 \n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 9\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 10\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 11\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]) # 12\n",
    "\n",
    "COLO_MOBILE_LIMITED = np.array([[1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                                [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                                [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                                [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                                [0, 0, 0, 0, 1, 1, 0, 0],\n",
    "                                [0, 0, 0, 0, 1, 1, 0, 0],\n",
    "                                [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "                                [0, 0, 0, 0, 0, 0, 1, 1]])\n",
    "COLO_MOBILE = np.zeros((25,25))\n",
    "for i in range(0, 10):\n",
    "    for x in range(0, 10):\n",
    "        COLO_MOBILE[i, x] = 1\n",
    "    COLO_MOBILE[i, 24] = 1\n",
    "    COLO_MOBILE[24, i] = 1\n",
    "for i in range(10, 17):\n",
    "    for x in range(10, 17):\n",
    "        COLO_MOBILE[i, x] = 1\n",
    "for i in range(17, 24):\n",
    "    for x in range(17, 24):\n",
    "        COLO_MOBILE[i, x] = 1\n",
    "\n",
    "\n",
    "# Also prepare colocation matrix for office\n",
    "COLO_OFFICE = np.zeros((24, 24))\n",
    "for i in range(0, 8):\n",
    "    for x in range(0, 8):\n",
    "        COLO_OFFICE[i, x] = 1\n",
    "for i in range(8, 16):\n",
    "    for x in range(8, 16):\n",
    "        COLO_OFFICE[i, x] = 1\n",
    "for i in range(16, 24):\n",
    "    for x in range(16, 24):\n",
    "        COLO_OFFICE[i, x] = 1\n",
    "\n",
    "INT_5s = '5sec/'\n",
    "INT_10s = '10sec/'\n",
    "INT_15s = '15sec/'\n",
    "INT_30s = '30sec/'\n",
    "INT_1min = '1min/'\n",
    "INT_2min = '2min/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a number of utility functions. First of all, we will need to be able to load data from the files on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(sensor1, sensor2, feature, jv, flip='01'):\n",
    "    if feature == 'noiseFingerprint':\n",
    "        if type(jv['results'][sensor1]['fingerprint_noise_lev1']) is str:\n",
    "            if sensor2 == flip:\n",
    "                return jv['results'][sensor1]['fingerprint_noise_lev1']\n",
    "            else:\n",
    "                return jv['results'][sensor1]['fingerprint_noise_lev2']\n",
    "        else:\n",
    "            if sensor2 == flip:\n",
    "                return ''.join([jv['results'][sensor1]['fingerprint_noise_lev1'][idx] for idx in sorted(jv['results'][sensor1]['fingerprint_noise_lev1'].keys())])\n",
    "            else:\n",
    "                return ''.join([jv['results'][sensor1]['fingerprint_noise_lev2'][idx] for idx in sorted(jv['results'][sensor1]['fingerprint_noise_lev2'].keys())])\n",
    "    if feature == 'lux_miettinen':\n",
    "        res = ''\n",
    "        i = 128\n",
    "        while i < len(jv[\"results\"].keys()):\n",
    "            x = sorted(jv[\"results\"].keys())[i-1]\n",
    "            res += jv[\"results\"][x]\n",
    "            i += 128\n",
    "        i = min(i, len(jv[\"results\"].keys()))\n",
    "        x = sorted(jv[\"results\"].keys())[i-1]\n",
    "        res += jv[\"results\"][x]\n",
    "        return res\n",
    "            \n",
    "\n",
    "def import_fps(sensors, scenario, feature, interval):\n",
    "    \"\"\"Load results from a specific file and return them as python dict.\"\"\"\n",
    "    rv = {}\n",
    "    if scenario == CAR_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-01/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "            for sen in sensors:\n",
    "                if sen == '01':\n",
    "                    path = pattern.format('02')\n",
    "                else:\n",
    "                    path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = []\n",
    "                    for res in j['results']:\n",
    "                        rv[sen].append(get_val(res, sen, feature, j))\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = [get_val(None, sen, feature, j)]\n",
    "    elif scenario == MOBILE_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-02/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "            for sen in sensors:\n",
    "                if sen == '02':\n",
    "                    path = pattern.format('03')\n",
    "                else:\n",
    "                    path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = []\n",
    "                    for res in j['results']:\n",
    "                        rv[sen].append(get_val(res, sen, feature, j, flip=\"02\"))\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = [get_val(None, sen, feature, j, flip=\"02\")]\n",
    "    if scenario == OFF_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            for day in ['1_0-24h', '2_24-48h', '3_48-72h', '4_72-96h','5_96-120h', '6_120-144h', '7_144-168h']:\n",
    "                pattern = BASE_DIR + scenario + 'audio/{}/Sensor-01/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "                for sen in sensors:\n",
    "                    if sen == '01':\n",
    "                        path = pattern.format(day, '02')\n",
    "                    else:\n",
    "                        path = pattern.format(day, sen)\n",
    "                    with gzip.open(path, 'rt') as fo:\n",
    "                        j = json.loads(fo.read())\n",
    "                        if sen not in rv:\n",
    "                            rv[sen] = []\n",
    "                        for res in j['results']:\n",
    "                            rv[sen].append(get_val(res, sen, feature, j))\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = [get_val(None, sen, feature, j)]\n",
    "    return rv\n",
    "\n",
    "\n",
    "def import_fps_timeslotted(sensors, scenario, feature, interval, bits=128):\n",
    "    \"\"\"Load results from a specific file and return them as python dict.\"\"\"\n",
    "    def increment_timeslot(ts, interval, count):\n",
    "        if interval == INT_5s:\n",
    "            incr = count*5\n",
    "        elif interval == INT_10s:\n",
    "            incr = count*10\n",
    "        elif interval == INT_15s:\n",
    "            incr = count*15\n",
    "        elif interval == INT_30s:\n",
    "            incr = count*30\n",
    "        elif interval == INT_1min:\n",
    "            incr = count*60\n",
    "        elif interval == INT_2min:\n",
    "            incr = count*120\n",
    "        else:\n",
    "            raise Exception(\"Wtf?!\")\n",
    "        return (ts + datetime.timedelta(seconds=incr))\n",
    "    \n",
    "    def base_time(ts, interval):\n",
    "        if interval == INT_5s:\n",
    "            decr = ts.second % 5\n",
    "        elif interval == INT_10s:\n",
    "            decr = ts.second % 10\n",
    "        elif interval == INT_15s:\n",
    "            decr = ts.second % 15\n",
    "        elif interval == INT_30s:\n",
    "            decr = ts.second % 30\n",
    "        elif interval == INT_1min:\n",
    "            decr = ts.second\n",
    "        elif interval == INT_2min:\n",
    "            decr = ts.second + (ts.minute % 2) * 60\n",
    "        else:\n",
    "            raise Exception(\"Wtf?!\")\n",
    "        return (ts - datetime.timedelta(seconds = decr))\n",
    "        \n",
    "    rv = {}\n",
    "    if scenario == CAR_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-01/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "            for sen in sensors:\n",
    "                if sen == '01':\n",
    "                    path = pattern.format('02')\n",
    "                else:\n",
    "                    path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = {}\n",
    "                    tstamp = list(j[\"results\"].keys())[0]\n",
    "                    fp = get_val(tstamp, sen, feature, j)\n",
    "                    ts = base_time(parser.parse(tstamp), interval)\n",
    "                    for i in range(1, len(fp)):\n",
    "                        idx = increment_timeslot(ts, interval, i)\n",
    "                        rv[sen][idx] = ''.join(reversed(fp[max(i-bits, 0):i]))  # Reverse bit order\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    if bits == 128:\n",
    "                        # Fingerprints are already 128 bits. Phew.\n",
    "                        rv[sen] = j[\"results\"]\n",
    "                    else:\n",
    "                        rv[sen] = {}\n",
    "                        # Requested FPs are more or less than 128 bits, merge 128 bit fingerprints to split again later\n",
    "                        fp = \"\"\n",
    "                        for i in range(128, len(j[\"results\"].keys()), 128):\n",
    "                            idx = list(j[\"results\"].keys())[i]\n",
    "                            fp += ''.join(reversed(j[\"results\"][idx]))\n",
    "                        offset = len(j[\"results\"].keys()) % 128\n",
    "                        # Grab leftover bits\n",
    "                        if offset > 0:\n",
    "                            idx_c = len(j[\"results\"].keys()) - 1\n",
    "                            idx = list(j[\"results\"].keys())[idx_c]\n",
    "                            fp += ''.join(reversed(j[\"results\"][idx][:offset]))\n",
    "                        \n",
    "                        # Generate the new fingerprints\n",
    "                        tstamp = sorted(j[\"results\"].keys())[0]\n",
    "                        ts = base_time(parser.parse(tstamp), interval)\n",
    "                        for i in range(1, len(fp)):\n",
    "                            idx = increment_timeslot(ts, interval, i)\n",
    "                            rv[sen][idx] = ''.join(reversed(fp[max(i-bits, 0):i]))  # Reverse bit order again\n",
    "                        \n",
    "    elif scenario == MOBILE_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-02/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "            for sen in sensors:\n",
    "                if sen == '02':\n",
    "                    path = pattern.format('03')\n",
    "                else:\n",
    "                    path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = {}\n",
    "                    tstamp = list(j[\"results\"].keys())[0]\n",
    "                    fp = get_val(tstamp, sen, feature, j, flip='02')\n",
    "                    ts = base_time(parser.parse(tstamp), interval)\n",
    "                    for i in range(1, len(fp)):\n",
    "                        idx = increment_timeslot(ts, interval, i)\n",
    "                        rv[sen][idx] = ''.join(reversed(fp[max(i-bits, 0):i]))  # Reverse bit order\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    if bits == 128:\n",
    "                        # Fingerprints are already 128 bits. Phew.\n",
    "                        rv[sen] = j[\"results\"]\n",
    "                    else:\n",
    "                        rv[sen] = {}\n",
    "                        # Requested FPs are more or less than 128 bits, merge 128 bit fingerprints to split again later\n",
    "                        fp = \"\"\n",
    "                        for i in range(128, len(j[\"results\"].keys()), 128):\n",
    "                            idx = list(j[\"results\"].keys())[i]\n",
    "                            fp += ''.join(reversed(j[\"results\"][idx]))\n",
    "                        offset = len(j[\"results\"].keys()) % 128\n",
    "                        # Grab leftover bits\n",
    "                        if offset > 0:\n",
    "                            idx_c = len(j[\"results\"].keys()) - 1\n",
    "                            idx = list(j[\"results\"].keys())[idx_c]\n",
    "                            fp += ''.join(reversed(j[\"results\"][idx][:offset]))\n",
    "                        \n",
    "                        # Generate the new fingerprints\n",
    "                        tstamp = sorted(j[\"results\"].keys())[0]\n",
    "                        ts = base_time(parser.parse(tstamp), interval)\n",
    "                        for i in range(1, len(fp)):\n",
    "                            idx = increment_timeslot(ts, interval, i)\n",
    "                            rv[sen][idx] = ''.join(reversed(fp[max(i-bits, 0):i]))  # Reverse bit order again\n",
    "                        \n",
    "\n",
    "    if scenario == OFF_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            for sen in sensors:\n",
    "                fp = ''\n",
    "                rv[sen] = {}\n",
    "                tstamp = None\n",
    "                for day in ['1_0-24h', '2_24-48h', '3_48-72h', '4_72-96h','5_96-120h', '6_120-144h', '7_144-168h']:\n",
    "                    pattern = BASE_DIR + scenario + 'audio/{}/Sensor-01/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "                    if sen == '01':\n",
    "                        path = pattern.format(day, '02')\n",
    "                    else:\n",
    "                        path = pattern.format(day, sen)\n",
    "                    with gzip.open(path, 'rt') as fo:\n",
    "                        j = json.loads(fo.read())\n",
    "                        if day == '1_0-24h':\n",
    "                            tstamp = list(j[\"results\"].keys())[0]\n",
    "                        fp += get_val(list(j[\"results\"].keys())[0], sen, feature, j)\n",
    "                \n",
    "                \n",
    "                ts = base_time(parser.parse(tstamp), interval)\n",
    "                for i in range(1, len(fp)):\n",
    "                    idx = increment_timeslot(ts, interval, i)\n",
    "                    rv[sen][idx] = ''.join(reversed(fp[max(i-bits, 0):i]))  # Reverse bit order\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    if bits == 128:\n",
    "                        # Fingerprints are already 128 bits\n",
    "                        rv[sen] = j[\"results\"]\n",
    "                    else:\n",
    "                        rv[sen] = {}\n",
    "                        # Requested FPs are more or less than 128 bits, merge 128 bit fingerprints\n",
    "                        fp = \"\"\n",
    "                        for i in range(128, len(j[\"results\"].keys()), 128):\n",
    "                            idx = list(j[\"results\"].keys())[i]\n",
    "                            fp += ''.join(reversed(j[\"results\"][idx]))\n",
    "                        offset = len(j[\"results\"].keys()) % 128\n",
    "                        # Grab leftover bits\n",
    "                        if offset > 0:\n",
    "                            idx_c = len(j[\"results\"].keys()) - 1\n",
    "                            idx = list(j[\"results\"].keys())[idx_c]\n",
    "                            fp += ''.join(reversed(j[\"results\"][idx][:offset]))\n",
    "                        \n",
    "                        # Generate new fingerprints\n",
    "                        tstamp = sorted(j[\"results\"].keys())[0]\n",
    "                        ts = base_time(parser.parse(tstamp), interval)\n",
    "                        for i in range(1, len(fp)):\n",
    "                            idx = increment_timeslot(ts, interval, i)\n",
    "                            rv[sen][idx] = ''.join(reversed(fp[max(i-bits, 0):i]))  # Reverse bit order again\n",
    "    return rv\n",
    "\n",
    "# Save the resulting JSON data from the get_far_frr function to a file\n",
    "def save_result_json(data, paper, interval, scenario, modality, subscenario=None, suffix=None):\n",
    "    \"\"\"Save a dictionary into a file as JSON. Mostly used for cache files.\n",
    "    \n",
    "    :param data: The data to save.\n",
    "    :param paper: The paper to save it under (SOUNDPROOF, ...)\n",
    "    :param interval: The interval to save it under\n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param modality: The modality (max_xcorr, ...)\n",
    "    :param subscenario: The subscenario, or None if global.\n",
    "    :param suffix: A suffix to place before the .json\"\"\"\n",
    "    path = '/'.join([PREFIX_JSON, scenario, paper, modality])\n",
    "    filename = path + '/' + interval\n",
    "    if subscenario is not None:\n",
    "        filename += '-' + subscenario\n",
    "    if suffix is not None:\n",
    "        filename += '_' + suffix\n",
    "    filename += '.json'\n",
    "    makedirs(path, exist_ok=True)\n",
    "    with open(filename, 'w') as fo:\n",
    "        json.dump(data, fo, separators=(',', ': '), indent=4)\n",
    "\n",
    "# Check if a result cache file (generated by save_result_json) exists\n",
    "def result_exists(paper, interval, scenario, modality, subscenario=None, suffix=None):\n",
    "    \"\"\"Check if a cache file for a set of parameters exists.\n",
    "    \n",
    "    :param paper: The paper (SOUNDPROOF, ...)\n",
    "    :param interval: The interval\n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param modality: The modality (max_xcorr, ...)\n",
    "    :param subscenario: The subscenario, or None if global.\n",
    "    :return: True if a file exists, otherwise False\"\"\"\n",
    "    path = '/'.join([PREFIX_JSON, scenario, paper, modality])\n",
    "    filename = path + '/' + interval\n",
    "    if subscenario is not None:\n",
    "        filename += '-' + subscenario\n",
    "    if suffix is not None:\n",
    "        filename += '_' + suffix\n",
    "    filename += '.json'\n",
    "    return isfile(filename)\n",
    "\n",
    "# Load result cache file (generated by save_result_json)\n",
    "def load_result(paper, interval, scenario, modality, subscenario=None, suffix=None):\n",
    "    \"\"\"Check if a cache file for a set of parameters exists.\n",
    "    \n",
    "    :param paper: The paper(SOUNDPROOF, ...)\n",
    "    :param interval: The interval\n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param modality: The modality (max_xcorr, ...)\n",
    "    :param subscenario: The subscenario, or None if global.\n",
    "    :return: The loaded cache as a dictionary\"\"\"\n",
    "    assert result_exists(paper, interval, scenario, modality, subscenario, suffix)\n",
    "    rv = {}\n",
    "    \n",
    "    # Construct file name\n",
    "    path = '/'.join([PREFIX_JSON, scenario, paper, modality])\n",
    "    filename = path + '/' + interval\n",
    "    if subscenario is not None:\n",
    "        filename += '-' + subscenario\n",
    "    if suffix is not None:\n",
    "        filename += '_' + suffix\n",
    "    filename += '.json'\n",
    "    \n",
    "    # Load the data\n",
    "    with open(filename, 'r') as fo:\n",
    "        r = json.load(fo)\n",
    "    \n",
    "    # Cast thresholds to float, if necessary\n",
    "    if suffix is None:\n",
    "        for threshold in r:\n",
    "            rv[float(threshold)] = r[threshold]\n",
    "        return rv\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of function to calculate error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def far_frr(colo, ncolo, maxv=100.0, minv=0.0, increments=1000):\n",
    "    res = {}\n",
    "    for i in range(increments+1):\n",
    "        true_acc = 0.0\n",
    "        false_acc = 0.0\n",
    "        true_rej = 0.0\n",
    "        false_rej = 0.0\n",
    "\n",
    "        threshold = minv + i * ((maxv - minv) / float(increments))\n",
    "        for value in colo:\n",
    "            if value >= threshold:\n",
    "                true_acc += 1\n",
    "            else:\n",
    "                false_rej += 1\n",
    "        for value in ncolo:\n",
    "            if value >= threshold:\n",
    "                false_acc += 1\n",
    "            else:\n",
    "                true_rej += 1\n",
    "\n",
    "        fpr = false_acc / (false_acc + true_rej)\n",
    "        fnr = false_rej / (false_rej + true_acc)\n",
    "        tpr = true_acc / (true_acc + false_rej)\n",
    "        tnr = true_rej / (true_rej + false_acc)\n",
    "        #print(threshold, fpr, fnr, tpr, tnr)\n",
    "        res[threshold] = {\"fpr\": fpr, \"fnr\": fnr, \"tpr\": tpr, \"tnr\": tnr}\n",
    "    return res\n",
    "\n",
    "\n",
    "def far_frr_plain(fingerprints, colo, maxv=100.0, minv=0.0, increments=1000, bits=128):\n",
    "    def histogram_intersection(h1, h2, bins):\n",
    "        bins = np.diff(bins)\n",
    "        sm = 0\n",
    "        for i in range(len(bins)):\n",
    "            sm += min(bins[i]*h1[i], bins[i]*h2[i])\n",
    "        return sm\n",
    "\n",
    "    rv = {}\n",
    "    colo_perc = []\n",
    "    ncolo_perc = []\n",
    "    for i in range(len(sensors)):\n",
    "        s1 = sensors[i]\n",
    "        if s1 not in rv:\n",
    "            rv[s1] = {}\n",
    "        for c in range(i+1, len(sensors)):\n",
    "            s2 = sensors[c]\n",
    "            #print(s1, s2)\n",
    "            if s2 not in rv:\n",
    "                rv[s2] = {}\n",
    "            res = {\"perc\": [], \"dates\": [], \"fp1\": [], \"fp2\": []}\n",
    "\n",
    "            for date in nfp[s1]:\n",
    "                if date not in nfp[s2]:\n",
    "                    continue\n",
    "                fp1 = nfp[s1][date]\n",
    "                fp2 = nfp[s2][date]\n",
    "                if len(fp1) != bits:\n",
    "                    continue\n",
    "                sim_perc = sim_percent(fp1, fp2)\n",
    "                if colo[i, c] == 1:\n",
    "                    colo_perc.append(sim_perc)\n",
    "                else:\n",
    "                    ncolo_perc.append(sim_perc)\n",
    "                \n",
    "                res[\"perc\"].append(sim_perc)\n",
    "                res[\"dates\"].append(date)\n",
    "                res[\"fp1\"].append((date, fp1))\n",
    "                res[\"fp2\"].append((date, fp2))\n",
    "            rv[s1][s2] = res\n",
    "            # rv[s2][s1] = res\n",
    "    \n",
    "    sns.kdeplot(colo_perc, label=\"Colocated\")\n",
    "    ax = sns.kdeplot(ncolo_perc, label=\"Non-Colocated\")\n",
    "    ax.set_xlim(0.0,100.0)\n",
    "\n",
    "    hc, bins = np.histogram(colo_perc, np.arange(0.0, 100.0, 1.0), density=True)\n",
    "    hn, _ = np.histogram(ncolo_perc, np.arange(0.0, 100.0, 1.0), density=True)\n",
    "    print(\"Intersection:\", histogram_intersection(hc, hn, bins))\n",
    "            \n",
    "    res = {}\n",
    "    for incr in range(increments+1):\n",
    "        threshold = minv + incr * ((maxv - minv) / float(increments))\n",
    "        res[threshold] = {}\n",
    "        \n",
    "        for i in range(len(sensors)):\n",
    "            s1 = sensors[i]\n",
    "            res[threshold][s1] = {}\n",
    "            for c in range(i+1, len(sensors)):\n",
    "                s2 = sensors[c]\n",
    "                res[threshold][s1][s2] = {\"ta\": 0.0, \"fa\": 0.0, \"tr\": 0.0, \"fr\": 0.0}\n",
    "                for s in range(len(rv[s1][s2][\"perc\"])):\n",
    "                    if colo[i, c] == 1:\n",
    "                        if rv[s1][s2][\"perc\"][s] >= threshold:\n",
    "                            res[threshold][s1][s2][\"ta\"] += 1\n",
    "                        else:\n",
    "                            res[threshold][s1][s2][\"fr\"] += 1\n",
    "                    else:\n",
    "                        if rv[s1][s2][\"perc\"][s] >= threshold:\n",
    "                            res[threshold][s1][s2][\"fa\"] += 1\n",
    "                        else:\n",
    "                            res[threshold][s1][s2][\"tr\"] += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "def far_frr_surprisal(fingerprints, colo, maxv=100.0, minv=0.0, increments=1000, bits=128, surprisal_margin=0.0):\n",
    "    probs = {}\n",
    "    for i in range(len(sensors)):\n",
    "        p = generate_probabilities(nfp[sensors[i]], bits)\n",
    "        probs[sensors[i]] = p\n",
    "\n",
    "    rv = {}\n",
    "    for i in range(len(sensors)):\n",
    "        s1 = sensors[i]\n",
    "        if s1 not in rv:\n",
    "            rv[s1] = {}\n",
    "        for c in range(i+1, len(sensors)):\n",
    "            s2 = sensors[c]\n",
    "            #print(s1, s2)\n",
    "            if s2 not in rv:\n",
    "                rv[s2] = {}\n",
    "            res = {\"perc\": [], \"dates\": [], \"fp1\": [], \"fp2\": []}\n",
    "\n",
    "            for date in nfp[s1]:\n",
    "                if date not in nfp[s2]:\n",
    "                    continue\n",
    "                fp1 = nfp[s1][date]\n",
    "                fp2 = nfp[s2][date]\n",
    "                if len(fp1) != bits:\n",
    "                    continue\n",
    "                res[\"perc\"].append(sim_percent(fp1, fp2))\n",
    "                res[\"dates\"].append(date)\n",
    "                res[\"fp1\"].append((date, fp1))\n",
    "                res[\"fp2\"].append((date, fp2))\n",
    "            rv[s1][s2] = res\n",
    "            # rv[s2][s1] = res\n",
    "\n",
    "    res = {}\n",
    "    for incr in range(increments+1):\n",
    "        incl_count = 0\n",
    "        threshold = minv + incr * ((maxv - minv) / float(increments))\n",
    "        res[threshold] = {}\n",
    "        \n",
    "        surprisal_threshold = bits * ((100.0 - threshold) / 100.0) + surprisal_margin\n",
    "        colo_perc = []\n",
    "        ncolo_perc = []\n",
    "        for i in range(len(sensors)):\n",
    "            s1 = sensors[i]\n",
    "            res[threshold][s1] = {}\n",
    "            for c in range(i+1, len(sensors)):\n",
    "                s2 = sensors[c]\n",
    "                res[threshold][s1][s2] = {\"ta\": 0.0, \"fa\": 0.0, \"tr\": 0.0, \"fr\": 0.0}\n",
    "                for s in range(len(rv[s1][s2][\"perc\"])):\n",
    "                    date, fp1 = rv[s1][s2][\"fp1\"][s]\n",
    "                    _, fp2 = rv[s1][s2][\"fp2\"][s]\n",
    "                    if surprisal(date, fp1, probs[s1]) < surprisal_threshold:\n",
    "                        continue\n",
    "                    if surprisal(date, fp2, probs[s2]) < surprisal_threshold:\n",
    "                        continue\n",
    "                    incl_count += 1\n",
    "                    if colo[i, c] == 1:\n",
    "                        if rv[s1][s2][\"perc\"][s] >= threshold:\n",
    "                            res[threshold][s1][s2][\"ta\"] += 1\n",
    "                        else:\n",
    "                            res[threshold][s1][s2][\"fr\"] += 1\n",
    "                    else:\n",
    "                        if rv[s1][s2][\"perc\"][s] >= threshold:\n",
    "                            res[threshold][s1][s2][\"fa\"] += 1\n",
    "                        else:\n",
    "                            res[threshold][s1][s2][\"tr\"] += 1\n",
    "        if incr == increments:\n",
    "            total = sum([len(nfp[x]) for x in nfp])\n",
    "            print(incl_count, \"/\", total)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def frr_for_far(data, target_far):\n",
    "    \"\"\"Calculate the False Reject Rate (FRR) implied by a given target False Accept Rate (FAR).\n",
    "    Can also compute this for subsets of the datasets, given by \"sources\" and \"targets\". In this\n",
    "    case, it will consider all combinations of sources and targets (e.g., if sources = [\"1\"] and\n",
    "    targets = [\"2\", \"3\"], it will consider 1-2 and 1-3, but not 2-3).\n",
    "    \n",
    "    :param data: The data as a dictionary, as produced by gen_far_frr or the import functions.\n",
    "    :param target_far: The false accept rate to aim for. Note that 1.0 implies 100%, so 0.1% should\n",
    "        be written as 0.001.\n",
    "    :return: A 3-tuple of observed FAR, FRR, and the used threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize previous values with bogus values to ensure they are never used in the first iteration.\n",
    "    prev_far = -500.0\n",
    "    prev_frr = -500.0\n",
    "    prev_threshold = -500.0\n",
    "    for threshold in data:\n",
    "        false_acc = 0.0\n",
    "        false_rej = 0.0\n",
    "        true_acc = 0.0\n",
    "        true_rej = 0.0\n",
    "        for s1 in data[threshold]:\n",
    "            for s2 in data[threshold][s1]:\n",
    "                # Include the numbers in the overall count\n",
    "                false_acc += data[threshold][s1][s2]['fa']\n",
    "                false_rej += data[threshold][s1][s2]['fr']\n",
    "                true_acc += data[threshold][s1][s2]['ta']\n",
    "                true_rej += data[threshold][s1][s2]['tr']\n",
    "        \n",
    "        # Calculate error rates\n",
    "        if false_acc + true_rej == 0:\n",
    "            # No valid solution exists\n",
    "            return None, None, None\n",
    "        far = false_acc / (false_acc + true_rej)\n",
    "        frr = false_rej / (false_rej + true_acc)\n",
    "        \n",
    "        if far > target_far:\n",
    "            # The computed FAR is above the target FAR. Save current values and carry on\n",
    "            prev_far = far\n",
    "            prev_frr = frr\n",
    "            prev_threshold = threshold\n",
    "        else:\n",
    "            # We have reached or passed the target FAR. Determine if the previous value was a better fit\n",
    "            if abs(target_far - far) < abs(target_far - prev_far):\n",
    "                # We are closer to the target FAR than the previous FAR, use our values\n",
    "                return (far, frr, threshold)\n",
    "            else:\n",
    "                # The previous values were closer, use them\n",
    "                return (prev_far, prev_frr, prev_threshold)\n",
    "    assert False, \"This statement should never be reached. Last error rates: FAR \" + str(prev_far) + \", FRR \" + str(prev_frr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FAR and FRR\n",
    "def plot_far_frr(results, xlow=0.0, xhigh=1.0):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(sorted(results.keys()), [results[threshold][\"fpr\"] for threshold in sorted(results.keys())], label='FAR')\n",
    "    ax.plot(sorted(results.keys()), [results[threshold][\"fnr\"] for threshold in sorted(results.keys())], label='FRR')\n",
    "    \n",
    "    prev_fnr = 100000\n",
    "    prev_fpr = 0\n",
    "    for threshold in sorted(results.keys()):\n",
    "        if results[threshold][\"fpr\"] <= results[threshold][\"fnr\"]:\n",
    "            fpr = results[threshold][\"fpr\"]\n",
    "            fnr = results[threshold][\"fnr\"]\n",
    "            if abs(prev_fpr - prev_fnr) < abs(results[threshold][\"fpr\"] - results[threshold][\"fnr\"]):\n",
    "                fpr = prev_fpr\n",
    "                fnr = prev_fnr\n",
    "                threshold = prev_thres\n",
    "            print(\"Thresh\", threshold, \"FAR\", fpr, \"FRR\", fnr)\n",
    "            ax.plot([threshold, threshold], [0.0, fnr], 'k-')\n",
    "            ax.plot([0.0, threshold], [fnr, fnr], 'k-')\n",
    "            ax.set(xlabel='threshold', ylabel='rate', xlim=(xlow,xhigh))\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "            return fpr, fnr, threshold\n",
    "        prev_fpr = results[threshold][\"fpr\"]\n",
    "        prev_fnr = results[threshold][\"fnr\"]\n",
    "        prev_thres = threshold\n",
    "    # There doesn't seem to be a crossover point\n",
    "    print(\"No crossover detected.\")\n",
    "    ax.set(xlabel='threshold', ylabel='rate', xlim=(xlow,xhigh))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_far_frr_surprisal(result_data, xlow=0.0, xhigh=1.0):\n",
    "    if result_data is None:\n",
    "        return\n",
    "    \n",
    "    results = {}\n",
    "    for threshold in result_data:\n",
    "        # Prepare result and temporary variables\n",
    "        results[threshold] = {}\n",
    "        true_acc = 0.0\n",
    "        true_rej = 0.0\n",
    "        false_acc = 0.0\n",
    "        false_rej = 0.0\n",
    "        \n",
    "        # Load counts into the temporary vars\n",
    "        for s1 in result_data[threshold]:\n",
    "            for s2 in result_data[threshold][s1]:\n",
    "                true_acc += result_data[threshold][s1][s2][\"ta\"]\n",
    "                true_rej += result_data[threshold][s1][s2][\"tr\"]\n",
    "                false_acc += result_data[threshold][s1][s2][\"fa\"]\n",
    "                false_rej += result_data[threshold][s1][s2][\"fr\"]\n",
    "        \n",
    "        # Calculate error rates\n",
    "        # False Accept Rate (FAR)\n",
    "        fpr = false_acc / (false_acc + true_rej)\n",
    "        # False Reject Rate (FRR)\n",
    "        fnr = false_rej / (false_rej + true_acc)\n",
    "        # True Accept Rate (TAR)\n",
    "        tpr = true_acc / (true_acc + false_rej)\n",
    "        # True Reject Rate (TRR)\n",
    "        tnr = true_rej / (true_rej + false_acc)\n",
    "\n",
    "        # Put them in a data structure the visualization function understands\n",
    "        results[threshold] = {\"fpr\": fpr, \"fnr\": fnr, \"tpr\": tpr, \"tnr\": tnr}\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(sorted(results.keys()), [results[threshold][\"fpr\"] for threshold in sorted(results.keys())], label='FAR')\n",
    "    ax.plot(sorted(results.keys()), [results[threshold][\"fnr\"] for threshold in sorted(results.keys())], label='FRR')\n",
    "    \n",
    "    prev_fnr = 100000\n",
    "    prev_fpr = 0\n",
    "    for threshold in sorted(results.keys()):\n",
    "        if results[threshold][\"fpr\"] <= results[threshold][\"fnr\"]:\n",
    "            fpr = results[threshold][\"fpr\"]\n",
    "            fnr = results[threshold][\"fnr\"]\n",
    "            # colo_perc = results[threshold][\"colo\"]\n",
    "            # ncolo_perc = results[threshold][\"ncolo\"]\n",
    "            \n",
    "            if abs(prev_fpr - prev_fnr) < abs(results[threshold][\"fpr\"] - results[threshold][\"fnr\"]):\n",
    "                fpr = prev_fpr\n",
    "                fnr = prev_fnr\n",
    "                threshold = prev_thres\n",
    "            \n",
    "            # hc, bins = np.histogram(colo_perc, np.arange(0.0, 100.0, 1.0), density=True)\n",
    "            # hn, _ = np.histogram(ncolo_perc, np.arange(0.0, 100.0, 1.0), density=True)\n",
    "            # print(\"Intersection:\", histogram_intersection(hc, hn, bins))\n",
    "            # print(\"Included pairs:\", len(ncolo_perc) + len(colo_perc))\n",
    "            \n",
    "            print(\"Thresh\", threshold, \"FAR\", fpr, \"FRR\", fnr, \"EER*\", (fpr + fnr) / 2.0)\n",
    "            ax.plot([threshold, threshold], [0.0, fnr], 'k-')\n",
    "            ax.plot([0.0, threshold], [fnr, fnr], 'k-')\n",
    "            ax.set(xlabel='threshold', ylabel='rate', xlim=(xlow,xhigh))\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "            # sns.kdeplot(colo_perc, label=\"Colocated\")\n",
    "            # ax = sns.kdeplot(ncolo_perc, label=\"Non-Colocated\")\n",
    "            # ax.set_xlim(0.0,100.0)\n",
    "            # plt.show()\n",
    "            return fpr, fnr, threshold\n",
    "        prev_fpr = results[threshold][\"fpr\"]\n",
    "        prev_fnr = results[threshold][\"fnr\"]\n",
    "        prev_thres = threshold\n",
    "    # There doesn't seem to be a crossover point\n",
    "    print(\"No crossover detected.\")\n",
    "    ax.set(xlabel='threshold', ylabel='rate', xlim=(xlow,xhigh))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Hamming Distance\n",
    "def hamming(str1, str2):\n",
    "    # Source: https://code.activestate.com/recipes/499304-hamming-distance/#c2\n",
    "    return sum(map(str.__ne__, str1, str2))\n",
    "\n",
    "def sim_percent(str1, str2):\n",
    "    ham = hamming(str1, str2)\n",
    "    return (1.0 - ham / float(min(len(str1), len(str2)))) * 100\n",
    "\n",
    "def histogram_intersection(h1, h2, bins):\n",
    "    bins = np.diff(bins)\n",
    "    sm = 0\n",
    "    for i in range(len(bins)):\n",
    "        sm += min(bins[i]*h1[i], bins[i]*h2[i])\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error rates\n",
    "At this point, we have defined all functions that we need for data analysis.\n",
    "\n",
    "To work with the FAR and FRR data, we need to first compute a series of accept and reject results for specific thresholds on the data. This is an expensive operation (on the order of up to a few days for large datasets like the office experiment), but it only needs to be run once - the results are cached on disk for future operations.\n",
    "\n",
    "If you have obtained this code together with the dataset, it should already contain these caches, no need to regenerate them unless you want to reproduce our results. The below code only generates them if they aren't found in the expected location.\n",
    "\n",
    "This code only works for the Car and Office scenarios, the Mobile scenario is computed using a different piece of code (further below), as it requires limiting processing to a specific timeframe in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = \"similarity_percent\"\n",
    "for PAPER in [\"noiseFingerprint\", \"lux_miettinen\"]:\n",
    "    for sensors, scenario, colo in zip([SENSORS_CAR, SENSORS_OFFICE], [CAR_EXP, OFF_EXP], [COLO_CAR, COLO_OFFICE]):\n",
    "        for interval in [INT_2min, INT_1min, INT_30s, INT_15s, INT_10s, INT_5s]:\n",
    "            for bits in [64, 128, 256, 512, 1024]:\n",
    "\n",
    "                print(bits, scenario, interval)\n",
    "                if not result_exists(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits)):\n",
    "                    nfp = import_fps_timeslotted(sensors, scenario, PAPER, interval, bits=bits)\n",
    "\n",
    "                    res = far_frr_plain(nfp, maxv=101.0, minv=60.0, increments=1000, bits=bits)\n",
    "\n",
    "                    save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits))\n",
    "\n",
    "                data = load_result(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits))\n",
    "\n",
    "                try:\n",
    "                    far, frr, threshold = plot_far_frr_surprisal(data, xlow=60.0, xhigh=100.0)\n",
    "                except ZeroDivisionError:\n",
    "                    print(\"Does not exist\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "                res = {\"base\": {\"eer\": {\"far\": far, \"frr\": frr, \"threshold\": threshold}}}\n",
    "\n",
    "                for target_far in [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05]:\n",
    "                    far, frr, threshold = frr_for_far(data, target_far)\n",
    "                    res[\"base\"][\"far_%s\" % target_far] = {\"far\": far, \"frr\": frr, \"threshold\": threshold}\n",
    "\n",
    "                # Save the whole thing\n",
    "                save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits), suffix=\"rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process for the mobile scenario, which requires slightly adapted processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = \"similarity_percent\"\n",
    "for PAPER in [\"noiseFingerprint\", \"lux_miettinen\"]:\n",
    "    for bits in [1024, 512, 256, 128, 64]:\n",
    "        for sensors, scenario, colo in zip([SENSORS_MOBILE], [MOBILE_EXP], [COLO_MOBILE]):\n",
    "            # for interval in [INT_5s, INT_10s, INT_15s, INT_30s, INT_1min, INT_2min]:\n",
    "            for interval in [INT_2min, INT_1min, INT_30s, INT_15s, INT_10s, INT_5s]:\n",
    "\n",
    "                print(bits, scenario, interval)\n",
    "                if not result_exists(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits)):\n",
    "                    nfp = import_fps_timeslotted(sensors, scenario, PAPER, interval, bits=bits)\n",
    "\n",
    "                    # After loading the data, we need to exclude all data points that are recorded after\n",
    "                    # the 21st of October 2018, 12:06:00. This is because after that point, the colocation\n",
    "                    # of devices changes, and long-term fingerprint-based systems break down.\n",
    "                    # We thus decided to only evaluate the first few hours of the recording.\n",
    "                    for sensor in nfp:\n",
    "                        to_del = []\n",
    "                        for ts in nfp[sensor].keys():\n",
    "                            x = ts\n",
    "                            # For some reason, we sometimes get a string instead of a datetime. Convert here.\n",
    "                            if not isinstance(ts, datetime.datetime):\n",
    "                                x = parser.parse(ts)\n",
    "                            if x > datetime.datetime(2018, 10, 21, 12, 6, 0):\n",
    "                                to_del.append(ts)\n",
    "                        for ts in to_del:\n",
    "                            del nfp[sensor][ts]\n",
    "                    \n",
    "                    # The rest of the code is identical to the one above.\n",
    "                    res = far_frr_plain(nfp, colo, maxv=101.0, minv=30.0, increments=1000, bits=bits)\n",
    "\n",
    "                    save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits))\n",
    "\n",
    "                data = load_result(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits))\n",
    "\n",
    "                try:\n",
    "                    far, frr, threshold = plot_far_frr_surprisal(data, xlow=30.0, xhigh=100.0)\n",
    "                except ZeroDivisionError:\n",
    "                    print(\"Does not exist\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "                res = {\"base\": {\"eer\": {\"far\": far, \"frr\": frr, \"threshold\": threshold}}}\n",
    "\n",
    "                for target_far in [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05]:\n",
    "                    far, frr, threshold = frr_for_far(data, target_far)\n",
    "                    res[\"base\"][\"far_%s\" % target_far] = {\"far\": far, \"frr\": frr, \"threshold\": threshold}\n",
    "\n",
    "                # Save the whole thing\n",
    "                save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits), suffix=\"rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprisal\n",
    "Miettinen et al. also propose an extension of their system that computes the \"surprisal\" of a fingerprint (i.e., how (un)predictable the fingerprint is), and discards those not meeting a predefined surprisal threshold. Due to the significant computational load of evaluating many different thresholds, we limit ourselves to a specific subset of thresholds.\n",
    "\n",
    "Once again, the code for the mobile scenario is kept separate.\n",
    "\n",
    "The following functions are used to generate the baseline probabilities and calculate the surprisal of specific fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_probabilities(fingerprints, bits=128):\n",
    "    prob = {}\n",
    "    # Go through all fingerprints\n",
    "    for tstamp in fingerprints:\n",
    "        fp = fingerprints[tstamp]\n",
    "        # Parse into datetime object and round down to hour\n",
    "        # Note: This is currently still day-specific, so the system is aware that the distribution\n",
    "        # on weekends will be different than on weekdays for the Office experiment\n",
    "        # If this is not desired, this needs to be changed here\n",
    "        ts = parser.parse(str(tstamp)).replace(microsecond=0,second=0,minute=0)\n",
    "        \n",
    "        # Check if we already have a sub-dictionary for this, if not, initialize it\n",
    "        if ts not in prob:\n",
    "            prob[ts] = {bit : 0.0 for bit in range(bits)}\n",
    "            prob[ts][\"count\"] = 0.0\n",
    "        \n",
    "        # Increment fp count for timeslot\n",
    "        prob[ts][\"count\"] += 1\n",
    "        \n",
    "        # Count up the ones in the fingerprint positions\n",
    "        for i in range(len(fp)):\n",
    "            if fp[i] == \"1\":\n",
    "                prob[ts][i] += 1\n",
    "    \n",
    "    # Now we have a finished state in the prob dictionary. Let's transform it into a distribution\n",
    "    for ts in prob.keys():\n",
    "        for i in range(bits):\n",
    "            prob[ts][i] /= prob[ts][\"count\"]\n",
    "    \n",
    "    return prob\n",
    "        \n",
    "\n",
    "def surprisal(tstamp, fp, probabilities):\n",
    "    # Parse out the time from the timestamp and round down\n",
    "    # See note above about being agnostic (or not) to weekends\n",
    "    ts = parser.parse(str(tstamp)).replace(microsecond=0,second=0,minute=0)\n",
    "    \n",
    "    # Compute the surprisal\n",
    "    s = 0\n",
    "    for i in range(len(fp)):\n",
    "        if fp[i] == \"1\":\n",
    "            s += -math.log(probabilities[ts][i], 2)\n",
    "        else:\n",
    "            s += -math.log(1 - probabilities[ts][i], 2)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the error rates when only considering fingerprints with a sufficient surprisal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = \"similarity_percent\"\n",
    "for PAPER in [\"lux_miettinen\", \"noiseFingerprint\"]:\n",
    "    for bits in [1024, 256, 64]:\n",
    "        for sensors, scenario, colo in zip([SENSORS_CAR, SENSORS_OFFICE], [CAR_EXP, OFF_EXP], [COLO_CAR, COLO_OFFICE]):\n",
    "            for interval in [INT_2min, INT_30s, INT_5s]:\n",
    "                skip_rest = False\n",
    "                # Define a number of different surprisal thresholds\n",
    "                for threshold in [0.2, 0.4, 0.6, 0.8]:\n",
    "                    if skip_rest:\n",
    "                        continue\n",
    "                    # Compute actual threshold\n",
    "                    surprisal_threshold = bits * threshold\n",
    "                    print(bits, scenario, interval, threshold)\n",
    "                    \n",
    "                    # If no cache exists, load and process the data\n",
    "                    if not result_exists(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold)):\n",
    "                        nfp = import_fps_timeslotted(sensors, scenario, PAPER, interval, bits=bits)\n",
    "\n",
    "                        res = far_frr_surprisal(nfp, colo, maxv=101.0, minv=30.0, increments=100, bits=bits, surprisal_margin=surprisal_threshold)\n",
    "\n",
    "                        save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold))\n",
    "\n",
    "                    data = load_result(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold))\n",
    "\n",
    "                    try:\n",
    "                        far, frr, threshold = plot_far_frr_surprisal(data, xlow=30.0, xhigh=100.0)\n",
    "                    except ZeroDivisionError:\n",
    "                        print(\"All data consumed, skipping rest.\")\n",
    "                        skip_rest = True\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    res = {\"base\": {\"eer\": {\"far\": far, \"frr\": frr, \"threshold\": threshold}}}\n",
    "\n",
    "                    #for target_far in [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05]:\n",
    "                    #    far, frr, threshold = frr_for_far(data, target_far)\n",
    "                    #    res[\"base\"][\"far_%s\" % target_far] = {\"far\": far, \"frr\": frr, \"threshold\": threshold}\n",
    "\n",
    "                    # Save the whole thing\n",
    "                    save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold), suffix=\"rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the separate code for the Mobile experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE = \"similarity_percent\"\n",
    "for PAPER in [\"lux_miettinen\", \"noiseFingerprint\"]:\n",
    "    for bits in [1024, 256, 64]:\n",
    "        for sensors, scenario, colo in zip([SENSORS_MOBILE], [MOBILE_EXP], [COLO_MOBILE]):\n",
    "            # for interval in [INT_5s, INT_10s, INT_15s, INT_30s, INT_1min, INT_2min]:\n",
    "            for interval in [INT_2min, INT_30s, INT_5s]:\n",
    "                skip_rest = False\n",
    "                for threshold in [0.0, 0.2, 0.5, 0.75, 1.0]:\n",
    "                    if skip_rest:\n",
    "                        continue\n",
    "                    surprisal_threshold = bits * threshold\n",
    "                    print(bits, scenario, interval, threshold)\n",
    "                    if not result_exists(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold)):\n",
    "                        nfp = import_fps_timeslotted(sensors, scenario, PAPER, interval, bits=bits)\n",
    "\n",
    "                        for sensor in nfp:\n",
    "                            to_del = []\n",
    "                            for ts in nfp[sensor].keys():\n",
    "                                x = ts\n",
    "                                # For some reason, we sometimes get a string instead of a datetime. Convert here.\n",
    "                                if not isinstance(ts, datetime.datetime):\n",
    "                                    x = parser.parse(ts)\n",
    "                                if x > datetime.datetime(2018, 10, 21, 12, 6, 0):\n",
    "                                    to_del.append(ts)\n",
    "                            for ts in to_del:\n",
    "                                del nfp[sensor][ts]\n",
    "\n",
    "                        res = far_frr_surprisal(nfp, colo, maxv=101.0, minv=30.0, increments=50, bits=bits, surprisal_margin=surprisal_threshold)\n",
    "\n",
    "                        save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold))\n",
    "\n",
    "                    data = load_result(PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold))\n",
    "\n",
    "                    try:\n",
    "                        far, frr, threshold = plot_far_frr_surprisal(data, xlow=30.0, xhigh=100.0)\n",
    "                    except ZeroDivisionError:\n",
    "                        print(\"All data consumed, skipping\")\n",
    "                        skip_rest = True\n",
    "                        continue\n",
    "\n",
    "                    res = {\"base\": {\"eer\": {\"far\": far, \"frr\": frr, \"threshold\": threshold}}}\n",
    "\n",
    "                    # Save the whole thing\n",
    "                    save_result_json(res, PAPER, interval[:-1], scenario, FEATURE, subscenario=\"bit-\" + str(bits) + \"-surpr-\" + str(threshold), suffix=\"rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness\n",
    "\n",
    "To compute the robustness of the scheme, we load the results and apply the thresholds of one to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPERS = ['lux_miettinen', 'noiseFingerprint']\n",
    "SCENARIOS = set([CAR_EXP, OFF_EXP, MOBILE_EXP])\n",
    "INTERVALS = [INT_2min, INT_1min, INT_30s, INT_15s, INT_10s, INT_5s]\n",
    "BITS = [64, 128, 256, 512, 1024]\n",
    "\n",
    "def error_rate_for_threshold(data, threshold):\n",
    "    \"\"\"Calculate the error rates when using a specific threshold.\n",
    "    \n",
    "    :param data: The data, as generated by gen_far_frr\n",
    "    :param threshold: The threshold to generate the error rates for\n",
    "    :return: A 2-tuple of far, frr\n",
    "    \"\"\"\n",
    "    false_acc = 0.0\n",
    "    false_rej = 0.0\n",
    "    true_acc = 0.0\n",
    "    true_rej = 0.0\n",
    "    try:\n",
    "        data[threshold]\n",
    "    except KeyError:\n",
    "        prev_thresh = 0.0\n",
    "        for thr in sorted(data.keys()):\n",
    "            if thr < threshold:\n",
    "                prev_thresh = thr\n",
    "            if thr > threshold:\n",
    "                # We just crossed the threshold. Check which of the two was\n",
    "                # closer to the target threshold\n",
    "                if abs(threshold - prev_thresh) < abs(threshold - thr):\n",
    "                    # previous threshold was closer\n",
    "                    threshold = prev_thresh\n",
    "                else:\n",
    "                    threshold = thr\n",
    "    for s1 in data[threshold]:\n",
    "        for s2 in data[threshold][s1]:\n",
    "            # Include the numbers in the overall count\n",
    "            false_acc += data[threshold][s1][s2]['fa']\n",
    "            false_rej += data[threshold][s1][s2]['fr']\n",
    "            true_acc += data[threshold][s1][s2]['ta']\n",
    "            true_rej += data[threshold][s1][s2]['tr']\n",
    "\n",
    "    # Calculate error rates\n",
    "    far = false_acc / (false_acc + true_rej)\n",
    "    frr = false_rej / (false_rej + true_acc)\n",
    "        \n",
    "    return (threshold, far, frr)\n",
    "\n",
    "for paper in PAPERS:\n",
    "    robustness_output = {}\n",
    "    for interval in INTERVALS:\n",
    "        \n",
    "        for scenario in SCENARIOS:\n",
    "            result = {scenario[:-1]: {}}\n",
    "            print(paper, interval, scenario)\n",
    "            for bit in BITS:\n",
    "                if bit not in robustness_output:\n",
    "                    robustness_output[bit] = {}\n",
    "                if not result_exists(paper, interval[:-1], scenario, \"similarity_percent\", \"bit-\" + str(bit), \"rates\"):\n",
    "                    # print(paper, interval, scenario, bit, \"does not exist\")\n",
    "                    continue\n",
    "\n",
    "                data = load_result(paper, interval[:-1], scenario, \"similarity_percent\", subscenario=\"bit-\" + str(bit), suffix=\"rates\")\n",
    "                orig_far = data[\"base\"][\"eer\"][\"far\"]\n",
    "                orig_frr = data[\"base\"][\"eer\"][\"frr\"]\n",
    "                data = load_result(paper, interval[:-1], scenario, \"similarity_percent\", subscenario=\"bit-\" + str(bit))\n",
    "                for target_scen in SCENARIOS - set([scenario]):\n",
    "                    if not result_exists(paper, interval[:-1], target_scen, \"similarity_percent\", \"bit-\" + str(bit), suffix=\"rates\"):\n",
    "                        continue\n",
    "                    \n",
    "                    target_data = load_result(paper, interval[:-1], target_scen, \"similarity_percent\", subscenario=\"bit-\" + str(bit), suffix=\"rates\")\n",
    "                    threshold = target_data[\"base\"][\"eer\"][\"threshold\"]\n",
    "                    threshold, far, frr = error_rate_for_threshold(data, threshold)\n",
    "                    result[scenario[:-1]][target_scen[:-1]] = {\n",
    "                        \"threshold\": threshold,\n",
    "                        \"far\": far,\n",
    "                        \"frr\": frr,\n",
    "                        \"orig_far\": orig_far,\n",
    "                        \"orig_frr\": orig_frr,\n",
    "                    }\n",
    "                    if scenario not in robustness_output[bit]:\n",
    "                        robustness_output[bit][scenario] = {target_scen: {}}\n",
    "                    if target_scen not in robustness_output[bit][scenario]:\n",
    "                        robustness_output[bit][scenario][target_scen] = {}\n",
    "                    \n",
    "                    try:\n",
    "                        far_change_rel = (orig_far / far) * 100\n",
    "                    except ZeroDivisionError:\n",
    "                        far_change_rel = np.nan\n",
    "                        \n",
    "                    try:\n",
    "                        frr_change_rel = (orig_frr / frr) * 100\n",
    "                    except ZeroDivisionError:\n",
    "                        frr_change_rel = np.nan\n",
    "\n",
    "                    robustness_output[bit][scenario][target_scen][interval] = {\n",
    "                        \"far\": far, \n",
    "                        \"frr\": frr,\n",
    "                        \"orig_far\": orig_far,\n",
    "                        \"orig_frr\": orig_frr,\n",
    "                        \"far_change_abs\": orig_far - far,\n",
    "                        \"frr_change_abs\": orig_frr - frr,\n",
    "                        \"total_change_abs\": abs(orig_far - far) + abs(orig_frr - frr),\n",
    "                        \"far_change_rel\": far_change_rel,\n",
    "                        \"frr_change_rel\": frr_change_rel\n",
    "                    }\n",
    "                # Save the result\n",
    "                save_result_json(result, paper, interval[:-1], scenario, \"similarity_percent\", 'bit-' + str(bit), \"robustness\")\n",
    "\n",
    "    for scenario in SCENARIOS:\n",
    "        for scenario2 in SCENARIOS - set([scenario]):\n",
    "            for bit in robustness_output:\n",
    "                # Print out table header\n",
    "                # \"scenario\" is the used dataset, scenario2 is the one whose threshold was used\n",
    "                print(\"Robustness\", bit, scenario, scenario2)\n",
    "                # What do these abbreviations mean?\n",
    "                # - Int = Interval that was used\n",
    "                # - FAR and FRR = Obtained false accept / reject rates\n",
    "                # - ofar and ofrr = Original FAR and FRR from the base scenario, for comparison\n",
    "                # - sprd = spread between FAR and FRR, i.e. abs(FAR - FRR)\n",
    "                # - osprd = Spread between original FAR and FRR, i.e. abs(ofar - ofrr)\n",
    "                # - aca and rca = Absolute change in false accept / reject rate, i.e. ofar - far\n",
    "                # - tca = Absolute changes summed up, i.e. aca + rca\n",
    "                # - acr and rcr = Relative changes in FAR and FRR, i.e. (ofar / far) * 100\n",
    "                # - oeer = original EER\n",
    "                # - eer = new EER, i.e. (far + frr) / 2.0\n",
    "                # - eerabs = absolute change in EER\n",
    "                print(\"Int\", \"FAR\", \"FRR\", \"ofar\", \"ofrr\", \"sprd\", \"osprd\", \"aca\", \"rca\", \"tca\", \"acr\", \"rcr\", \"oeer\", \"eer\", \"eerabs\", sep='\\t|')\n",
    "                print(\"-\" + \"-------+\"*14 + \"-------\")\n",
    "                scenpair = robustness_output[bit][scenario][scenario2]\n",
    "                # Ensure that we actually have data for that bit-scenario-scenario2 set\n",
    "                for interval in INTERVALS:\n",
    "                    try:\n",
    "                        scenpair[interval]['far']\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    oeer = (scenpair[interval]['orig_far'] + scenpair[interval]['orig_frr']) / 2.0\n",
    "                    eer = (scenpair[interval]['far'] + scenpair[interval]['frr']) / 2.0\n",
    "                    ospread = abs(scenpair[interval]['orig_far'] - scenpair[interval]['orig_frr'])\n",
    "                    spread = abs(scenpair[interval]['far'] - scenpair[interval]['frr'])\n",
    "                    print(interval, \n",
    "                          round(scenpair[interval]['far'], 3), \n",
    "                          round(scenpair[interval]['frr'], 3), \n",
    "                          round(scenpair[interval]['orig_far'], 3), \n",
    "                          round(scenpair[interval]['orig_frr'], 3), \n",
    "                          round(spread, 3),\n",
    "                          round(ospread, 3),\n",
    "                          round(scenpair[interval]['far_change_abs'], 3), \n",
    "                          round(scenpair[interval]['frr_change_abs'], 3),\n",
    "                          round(scenpair[interval]['total_change_abs'], 3),\n",
    "                          round(scenpair[interval]['far_change_rel'], 1), \n",
    "                          round(scenpair[interval]['frr_change_rel'], 1),\n",
    "                          round(oeer, 3),\n",
    "                          round(eer, 3),\n",
    "                          round(oeer - eer, 3),\n",
    "                          sep='\\t|')\n",
    "                print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomness\n",
    "To evaluate the randomness of the generated fingerprints, we use techniques developed by Brüsch et al. in their paper [\"On the Secrecy of Publicly Observable Biometric Features: Security Properties of Gait for Mobile Device Pairing\" (CoRR abs/1804.03997)](https://arxiv.org/abs/1804.03997)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on code by Arne Brüsch, cf. https://github.com/abruesch/randomness-figures\n",
    "# It has been adapted to fit our use case. For the details on the original concept, see:\n",
    "# Arne Brüsch, Ngu Nguyen, Dominik Schürmann, Stephan Sigg, and Lars Wolf. 2018. \n",
    "# On the Secrecy of Publicly Observable Biometric Features: Security Properties of Gait for \n",
    "# Mobile Device Pairing. CoRR abs/1804.03997 (2018). https://arxiv.org/abs/1804.03997\n",
    "# The original code is licensed under the GPLv3.\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from scipy.misc import comb\n",
    "from scipy.special import binom\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "def random_walk(key, sum_distribution, transition_count, transition_probs, bitlength=128):\n",
    "    ''' takes string of '0' and '1' and turns them into random walks in a galton board. Effectively computes\n",
    "    the cumulative sums distribution for every prefix of the input string. '''\n",
    "\n",
    "    transition_count[0] += 1\n",
    "    ret = [0]\n",
    "    for i, b in enumerate(key[:bitlength]):\n",
    "        val = -1\n",
    "        if b == '1':\n",
    "            val = 1\n",
    "            transition_probs[i] += 1\n",
    "        ret.append(ret[-1] + val)\n",
    "\n",
    "    sum_distribution.append(ret[-1])\n",
    "    return (sum_distribution, transition_count, transition_probs)\n",
    "\n",
    "\n",
    "def markov_transitions(transition_probs, transition_count, bits=128):\n",
    "    ''' transition probabilities from every nth to (n+1)th bit. '''\n",
    "    norm_transition_probs = [transition_probs[x] / transition_count[0]\n",
    "                             for x in range(0, bits)]\n",
    "    plt.clf()\n",
    "    plt.xlabel('nth bit')\n",
    "    plt.ylabel('Probability for 1')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, bits])\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.plot(norm_transition_probs, color='b')\n",
    "    plt.plot([0.0, bits], [0.5, 0.5], 'k:')\n",
    "    plt.show()\n",
    "    print(\"Markov:\", np.median(norm_transition_probs))\n",
    "\n",
    "from scipy.misc import comb\n",
    "\n",
    "def binomial_plot(n=256):\n",
    "    ''' Theoretical binomial distribution that is plotted as red line into figure.'''\n",
    "    x = np.arange(n)\n",
    "    # y = list(map(lambda xi: comb(n,xi)/2**n, x))\n",
    "    y = list(binom(n,x)*0.5**x*(0.5)**(n-x))\n",
    "    x = list(map(lambda r: r-n/2, x))\n",
    "    plt.plot(x,y,color='r')\n",
    "\n",
    "#def binomial_plot(bins,n):\n",
    "#    variance =  n * 0.5 * 0.5\n",
    "#    sigma = np.sqrt(variance)\n",
    "#    y = mlab.normpdf(np.asarray(list(range(-128,128))), 0, sigma)\n",
    "#    plt.plot(np.asarray(list(range(-128,128))),y,color='r')\n",
    "\n",
    "def distribution(sum_distribution, bits=128, dist_xlim=None, save_to=None):\n",
    "    ''' Plots the cumulative sums distribution and saves figure. '''\n",
    "    plt.clf()\n",
    "    if dist_xlim is not None:\n",
    "        plt.xlim(dist_xlim)\n",
    "    else:\n",
    "        plt.xlim([-bits, bits])\n",
    "    count, bins, ignored = plt.hist(sum_distribution, color='#007a9b', range=(-bits,bits), normed=True,rwidth=0.5, bins=bits)\n",
    "    binomial_plot(bits*2)\n",
    "    # Get the current axes\n",
    "    ax = plt.gca()\n",
    "    # Get limits\n",
    "    start, end = ax.get_xlim()\n",
    "    if abs(start) > 150:\n",
    "        # We are appearantly plotting a very long fingerprint, let's make the ticks more sparse\n",
    "        ax.xaxis.set_ticks(np.arange(start, end+1, 100))\n",
    "        \n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    \n",
    "    if save_to is not None:\n",
    "        plt.savefig(save_to, format='eps', dpi=1000)\n",
    "    plt.show()\n",
    "    print(\"Median of distribution:\", np.median(sum_distribution))\n",
    "    \n",
    "\n",
    "\n",
    "def plot_rand_walk(keys, bits=128, dist_xlim=None, save_distribution_to=None):\n",
    "    ''' turns string consisting of '0' and '1' into random walks. While the walks are currently not plotted, the distribution\n",
    "    of the cumulative sums along with the markov transitions are computed and plotted using them.'''\n",
    "    sum_distribution = []\n",
    "    transition_count = [0]\n",
    "    transition_probs = {x: 0 for x in range(0, bits)}\n",
    "\n",
    "    matplotlib.rcParams['axes.prop_cycle'] = cycler('color',\n",
    "                                                    ['#e78a33', '#eda766', '#8d4959', '#aa7782', '#bdcd61', '#cdda89',\n",
    "                                                     '#8a9c33', '#a7b566'])\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "    plt.ylabel('Sum')\n",
    "    plt.xlabel('Keylength')\n",
    "\n",
    "    plt.ylim([-bits, bits])\n",
    "    for key in keys:\n",
    "        sum_distribution, transition_count, transition_probs = random_walk(key, sum_distribution, transition_count, transition_probs, bits)\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    distribution(sum_distribution, bits, dist_xlim=dist_xlim, save_to=save_distribution_to)\n",
    "    markov_transitions(transition_probs, transition_count, bits)\n",
    "\n",
    "\n",
    "def plot_heat_map(keys, pt=plt):\n",
    "    ''' creates heatmap of random walks.'''\n",
    "    intensity = 1\n",
    "    heatmap_array = []\n",
    "    for key in keys:\n",
    "        val = int(key[0])\n",
    "        for i, bit in enumerate(key[1:128]):\n",
    "            if bit == '1':\n",
    "                val = val + 1\n",
    "            elif bit == '0':\n",
    "                val = val - 1\n",
    "            heatmap_array.append([i, val])\n",
    "\n",
    "    np_srt_heat = np.asarray(heatmap_array)\n",
    "    X = np.take(np_srt_heat, [0], axis=1).flatten()\n",
    "    Y = np.take(np_srt_heat, [1], axis=1).flatten()\n",
    "    # bins = (range(max(X)), range(min(Y),max(Y)))\n",
    "    bins = (range(128), range(min(Y), max(Y)))\n",
    "    H, xedges, yedges = np.histogram2d(X, Y, bins=bins, normed=False)\n",
    "    # H = H.T\n",
    "    # H = H\n",
    "    H = intensity * H\n",
    "    # 'viridis'\n",
    "    pt.xlabel('Sum')\n",
    "    pt.ylabel('Keylength')\n",
    "    pt.imshow(H, cmap='viridis', norm=matplotlib.colors.LogNorm(), interpolation='nearest', origin='upper',\n",
    "              extent=[yedges[0], yedges[-1], xedges[-1], xedges[0]])\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    # Plot triangle\n",
    "    plus_y = [i for i in range(-1, 127)]\n",
    "    plus_x = [i for i in range(1, 129)]\n",
    "    minus_y = [126 - i for i in range(0, 128)]\n",
    "    minus_x = [-127 + i for i in range(1, 129)]\n",
    "    plt.tight_layout()\n",
    "    pt.plot(plus_x, plus_y, color='r')\n",
    "    pt.plot(minus_x, minus_y, color='r')\n",
    "\n",
    "\n",
    "def apply_plot_heat_map(keys):\n",
    "    ''' Calls plot_heat_map() and additionally saves the plotted figure.'''\n",
    "    plot_heat_map(keys)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the functions defined above to evaluate and plot the bit distributions for the schemes in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper, modality in [('noiseFingerprint', 'audio'), ('lux_miettinen', 'lux')]:\n",
    "    for scenario, sensors in [(CAR_EXP, SENSORS_CAR), (OFF_EXP, SENSORS_OFFICE), (MOBILE_EXP, SENSORS_MOBILE)]:\n",
    "        for interval in [INT_5s, INT_10s, INT_15s, INT_30s, INT_1min, INT_2min]:\n",
    "            for bits in [128]:\n",
    "                a = import_fps_timeslotted(sensors, scenario, 'noiseFingerprint', interval, bits=bits)\n",
    "                for i in a.keys():\n",
    "                    keys = a[i].values()\n",
    "                    print(paper, scenario, interval, bits, i)\n",
    "                    plot_rand_walk(keys, bits=bits, save_distribution_to='/home/seemoo/plots/img/%s%s/%s/sensor-%s-%s-%s.eps' % (scenario, paper, modality, i, bits, interval[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
