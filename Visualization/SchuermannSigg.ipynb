{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schürmann and Sigg\n",
    "===========\n",
    "\n",
    "This notebook is used to generate the results for the evaluation of the scheme by Schürmann and Sigg (Schürmann, Dominik, and Stephan Sigg. \"Secure communication based on ambient audio.\" IEEE Transactions on mobile computing 12, no. 2 (2013): 358-370.). In the code, this scheme is called \"audioFingerprint\", in the paper it is evaluated in section 4.2.\n",
    "\n",
    "First, we have to import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import copy\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "import seaborn as sns\n",
    "from itertools import zip_longest, combinations\n",
    "from os import makedirs\n",
    "from os.path import isfile\n",
    "import math\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a series of constants so that the system can find all files. Change the paths to whatever works on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File system constants\n",
    "PREFIX=\"/media/seemoo/data/zia-data/results/\"\n",
    "PREFIX_RAW=\"/media/seemoo/data/zia-data/raw/\"\n",
    "\n",
    "PREFIX_PLOTS='/home/seemoo/plots/img'\n",
    "PREFIX_JSON='/home/seemoo/plots/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional internal constants for use in the code, leave these alone unless you know what you are doing. They are documented in more detail in the Sound-Proof notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenarios\n",
    "S_CAR = 'CarExp'\n",
    "S_OFFICE = 'OfficeExp'\n",
    "S_MOBILE = 'MobileExp'\n",
    "\n",
    "# Subscenarios\n",
    "SU_PARKED = 'parked'\n",
    "SU_CITY = 'city'\n",
    "SU_HIGHWAY = 'highway'\n",
    "\n",
    "SU_WDAY = 'weekday'\n",
    "SU_NIGHT = 'night'\n",
    "SU_WEND = 'weekend'\n",
    "\n",
    "# Sensor lists\n",
    "CAR_SENSORS1 = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\"]\n",
    "CAR_SENSORS2 = [\"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "OFFICE_SENSORS1 = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"]\n",
    "OFFICE_SENSORS2 = [\"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\"]\n",
    "OFFICE_SENSORS3 = [\"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\"]\n",
    "\n",
    "\n",
    "# Features\n",
    "TIME_FREQ_DIST = 'timeFreqDistance'\n",
    "NOISE_FP = 'noiseFingerprint'\n",
    "SOUNDPROOF = 'soundProofXcorr'\n",
    "AUDIO_FP = 'audioFingerprint'\n",
    "\n",
    "TRUONG = 'ble_wifi_truong'\n",
    "SHRESTHA = 'temp_hum_press_shrestha'\n",
    "\n",
    "\n",
    "# Intervals\n",
    "INTERVALS = ['5sec', '10sec', '15sec', '30sec', '1min', '2min']\n",
    "# Colocation arrays\n",
    "# These arrays are used to represent which sensors were considered colocated\n",
    "# in the two scenarios.\n",
    "#                     1  2  3  4  5  6  7  8  9 10 11 12\n",
    "COLO_CAR = np.array([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 1\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 2\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 3\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 4\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 5\n",
    "                     [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],  # 6\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 7\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 8 \n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 9\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 10\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],  # 11\n",
    "                     [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]) # 12\n",
    "\n",
    "# Also prepare colocation matrix for office, using a script to make it more concise.\n",
    "COLO_OFFICE = np.zeros((24, 24))\n",
    "for i in range(0, 8):\n",
    "    for x in range(0, 8):\n",
    "        COLO_OFFICE[i, x] = 1\n",
    "for i in range(8, 16):\n",
    "    for x in range(8, 16):\n",
    "        COLO_OFFICE[i, x] = 1\n",
    "for i in range(16, 24):\n",
    "    for x in range(16, 24):\n",
    "        COLO_OFFICE[i, x] = 1\n",
    "\n",
    "# Colocation information for mobile sensor scenario\n",
    "COLO_MOBILE = {}\n",
    "COLO_MOBILE[2] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "COLO_MOBILE[3] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "COLO_MOBILE[4] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[5] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 13, 51, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 13, 51, 0), datetime(2018, 10, 21, 13, 55, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 13, 55, 0), datetime(2018, 10, 21, 14, 2, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 14, 2, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                  (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 16, 7, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 16, 7, 0), datetime(2018, 10, 21, 16, 9, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 9, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[6] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 13, 51, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 13, 51, 0), datetime(2018, 10, 21, 13, 55, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 13, 55, 0), datetime(2018, 10, 21, 14, 2, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 14, 2, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                  (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 16, 7, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 16, 7, 0), datetime(2018, 10, 21, 16, 9, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 9, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[7] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 14, 2, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 14, 2, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                  (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[8] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 14, 12, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 14, 12, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                  (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 16, 16, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 16, 16, 0), datetime(2018, 10, 21, 16, 25, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 25, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[9] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 14, 12, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 14, 12, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                  (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 16, 16, 0), 1),\n",
    "                  (datetime(2018, 10, 21, 16, 16, 0), datetime(2018, 10, 21, 16, 25, 0), 2),\n",
    "                  (datetime(2018, 10, 21, 16, 25, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[10] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 14, 20, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 14, 20, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[11] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 2)]\n",
    "COLO_MOBILE[12] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 2)]\n",
    "COLO_MOBILE[13] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 2)]\n",
    "COLO_MOBILE[14] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 2)]\n",
    "\n",
    "COLO_MOBILE[15] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 10, 48, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 10, 48, 0), datetime(2018, 10, 21, 10, 52, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 10, 52, 0), datetime(2018, 10, 21, 12, 9, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 12, 9, 0), datetime(2018, 10, 21, 12, 49, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 12, 49, 0), datetime(2018, 10, 21, 14, 17, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 14, 17, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 16, 25, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 16, 25, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[16] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 9, 28, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 9, 28, 0), datetime(2018, 10, 21, 10, 48, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 10, 48, 0), datetime(2018, 10, 21, 10, 52, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 10, 52, 0), datetime(2018, 10, 21, 12, 9, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 12, 9, 0), datetime(2018, 10, 21, 12, 49, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 12, 49, 0), datetime(2018, 10, 21, 14, 17, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 14, 17, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 16, 25, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 16, 25, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[17] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 14, 17, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 14, 17, 0), datetime(2018, 10, 21, 15, 0, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 15, 0, 0), datetime(2018, 10, 21, 16, 4, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 16, 4, 0), datetime(2018, 10, 21, 16, 35, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 16, 35, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "COLO_MOBILE[18] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 3)]\n",
    "COLO_MOBILE[19] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 3)]\n",
    "COLO_MOBILE[20] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 3)]\n",
    "COLO_MOBILE[21] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 3)]\n",
    "\n",
    "COLO_MOBILE[22] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 9, 28, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 9, 28, 0), datetime(2018, 10, 21, 12, 13, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 12, 13, 0), datetime(2018, 10, 21, 12, 46, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 12, 46, 0), datetime(2018, 10, 21, 17, 30, 0), 3)]\n",
    "\n",
    "COLO_MOBILE[23] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 9, 28, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 9, 28, 0), datetime(2018, 10, 21, 12, 13, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 12, 13, 0), datetime(2018, 10, 21, 12, 46, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 12, 46, 0), datetime(2018, 10, 21, 17, 30, 0), 3)]\n",
    "\n",
    "COLO_MOBILE[24] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 17, 30, 0), 3)]\n",
    "\n",
    "COLO_MOBILE[25] = [(datetime(2018, 10, 21, 8, 30, 0), datetime(2018, 10, 21, 14, 5, 0), 1),\n",
    "                   (datetime(2018, 10, 21, 14, 5, 0), datetime(2018, 10, 21, 15, 1, 0), 2),\n",
    "                   (datetime(2018, 10, 21, 15, 1, 0), datetime(2018, 10, 21, 16, 7, 0), 3),\n",
    "                   (datetime(2018, 10, 21, 16, 7, 0), datetime(2018, 10, 21, 17, 30, 0), 1)]\n",
    "\n",
    "\n",
    "# List all available results with their respective available parameters\n",
    "RESULTS = {\n",
    "    NOISE_FP: {\n",
    "        \"intervals\": ['5sec', '10sec', '15sec', '30sec', '1min', '2min'],\n",
    "        \"modalities\": [\"audio\"],\n",
    "        \"features\": [\"fingerprints_similarity_percent\"]\n",
    "    },\n",
    "    SOUNDPROOF: {\n",
    "        \"intervals\": ['5sec', '10sec', '15sec', '30sec', '1min', '2min'],\n",
    "        \"modalities\": [\"audio\"],\n",
    "        \"features\": [\"max_xcorr\"]\n",
    "    },\n",
    "    AUDIO_FP: {\n",
    "        \"intervals\": ['5sec', '10sec', '15sec', '30sec', '1min', '2min'],\n",
    "        \"modalities\": [\"audio\"],\n",
    "        \"features\": [\"fingerprints_similarity_percent\"]\n",
    "    },\n",
    "    TIME_FREQ_DIST: {\n",
    "        \"intervals\": ['5sec', '10sec', '15sec', '30sec', '1min', '2min'],\n",
    "        \"modalities\": [\"audio\"],\n",
    "        \"features\": [\"max_xcorr\", \"time_freq_dist\"]\n",
    "    },\n",
    "    SHRESTHA: {\n",
    "        \"intervals\": [\".\"],\n",
    "        \"modalities\": [\"temp\", \"hum\", \"press\"],\n",
    "        \"features\": [\"hamming_dist\"]\n",
    "    },\n",
    "    TRUONG: {\n",
    "        \"intervals\": ['10sec', '30sec'],\n",
    "        \"modalities\": [\"ble\", \"wifi\"],\n",
    "        \"features\": [\"euclidean\", \"jaccard\", \"mean_exp\", \"mean_hamming\", \"sum_squared_ranks\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first set up a few utility functions.\n",
    "\n",
    "Functions for loading files, and their helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File loading\n",
    "def load_file(path):\n",
    "    \"\"\"Load results from a specific file and return them as python dict.\"\"\"\n",
    "    if path.endswith('.gz'):\n",
    "        with gzip.open(path, 'rt') as fo:\n",
    "            j = json.loads(fo.read())\n",
    "            return j['results']\n",
    "    else:\n",
    "        with open(path, 'rt') as fo:\n",
    "            j = json.loads(fo.read())\n",
    "            return j['results']\n",
    "\n",
    "\n",
    "# Generate a path to a result file\n",
    "def generate_path(scenario, modality, feature, interval, sensor1, sensor2, day=None):\n",
    "    def pad_sensor(sensor):\n",
    "        return \"Sensor-\" + str(sensor).zfill(2)\n",
    "    if day is None:\n",
    "        return '/'.join([PREFIX, scenario, pad_sensor(sensor1), modality, feature, interval, pad_sensor(sensor2) + \".json.gz\"])\n",
    "    else:\n",
    "        return '/'.join([PREFIX, scenario, day, pad_sensor(sensor1), modality, feature, interval, pad_sensor(sensor2) + \".json.gz\"])\n",
    "\n",
    "\n",
    "def generate_summary_path(scenario, modality, feature, interval, sensor1, day=None, subscenario=None):\n",
    "    def pad_sensor(sensor):\n",
    "        return \"Sensor-\" + str(sensor).zfill(2)\n",
    "    if subscenario is not None:\n",
    "        filename = \"Summary-{}.json.gz\".format(subscenario)\n",
    "    else:\n",
    "        filename = \"Summary.json.gz\"\n",
    "    if day is None:\n",
    "        return '/'.join([PREFIX, scenario, pad_sensor(sensor1), modality, feature, interval, filename])\n",
    "    else:\n",
    "        return '/'.join([PREFIX, scenario, day, pad_sensor(sensor1), modality, feature, interval, filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out subscenarios\n",
    "def is_subscenario(scenario, subscenario, dt_tstamp):\n",
    "    \"\"\"Helper function to determine if a specific provided time is in a specific subscenario.\n",
    "    \n",
    "    :param scenario: The scenario (office or car)\n",
    "    :param subscenario: The subscenario we are interested in\n",
    "    :param dt_tstamp: The datetime object that should be checked.\n",
    "    :return: True if dt is within the subscenario timeframe, else False.\"\"\"\n",
    "    if subscenario is None:\n",
    "        return True\n",
    "    if scenario == S_CAR:\n",
    "        INCLUDE_INTERVALS = [(datetime(2017, 11, 23, 14, 40, 0), datetime(2017, 11, 23, 14, 46, 0), SU_PARKED),\n",
    "                             (datetime(2017, 11, 23, 14, 46, 0), datetime(2017, 11, 23, 15, 15, 0), SU_CITY),\n",
    "                             (datetime(2017, 11, 23, 15, 15, 0), datetime(2017, 11, 23, 15, 18, 0), SU_PARKED),\n",
    "                             (datetime(2017, 11, 23, 15, 18, 0), datetime(2017, 11, 23, 15, 55, 0), SU_HIGHWAY),\n",
    "                             (datetime(2017, 11, 23, 15, 55, 0), datetime(2017, 11, 23, 16, 25, 0), SU_CITY),\n",
    "                             (datetime(2017, 11, 23, 16, 25, 0), datetime(2017, 11, 23, 16, 43, 0), SU_HIGHWAY),\n",
    "                             (datetime(2017, 11, 23, 16, 43, 0), datetime(2017, 11, 23, 17, 5, 0), SU_PARKED),\n",
    "                             (datetime(2017, 11, 23, 17, 5, 0), datetime(2017, 11, 23, 17, 18, 0), SU_HIGHWAY),\n",
    "                             (datetime(2017, 11, 23, 17, 18, 0), datetime(2017, 11, 23, 17, 31, 0), SU_CITY),\n",
    "                             (datetime(2017, 11, 23, 17, 31, 0), datetime(2017, 11, 23, 17, 50, 0), SU_PARKED)]\n",
    "    elif scenario == S_OFFICE:\n",
    "        INCLUDE_INTERVALS = [(datetime(2017, 11, 27, 8, 0, 0), datetime(2017, 11, 27, 21, 0, 0), SU_WDAY),\n",
    "                             (datetime(2017, 11, 27, 21, 0, 0), datetime(2017, 11, 28, 8, 0, 0), SU_NIGHT),\n",
    "                             (datetime(2017, 11, 28, 8, 0, 0), datetime(2017, 11, 28, 21, 0, 0), SU_WDAY),\n",
    "                             (datetime(2017, 11, 28, 21, 0, 0), datetime(2017, 11, 29, 8, 0, 0), SU_NIGHT),\n",
    "                             (datetime(2017, 11, 29, 8, 0, 0), datetime(2017, 11, 29, 21, 0, 0), SU_WDAY),\n",
    "                             (datetime(2017, 11, 29, 21, 0, 0), datetime(2017, 11, 30, 8, 0, 0), SU_NIGHT),\n",
    "                             (datetime(2017, 11, 30, 8, 0, 0), datetime(2017, 11, 30, 21, 0, 0), SU_WDAY),\n",
    "                             (datetime(2017, 11, 30, 21, 0, 0), datetime(2017, 12, 1, 8, 0, 0), SU_NIGHT),\n",
    "                             (datetime(2017, 12, 1, 8, 0, 0), datetime(2017, 12, 1, 21, 0, 0), SU_WDAY),\n",
    "                             (datetime(2017, 12, 1, 21, 0, 0), datetime(2017, 12, 2, 8, 0, 0), SU_NIGHT),\n",
    "                             (datetime(2017, 12, 2, 8, 0, 0), datetime(2017, 12, 2, 21, 0, 0), SU_WEND),\n",
    "                             (datetime(2017, 12, 2, 21, 0, 0), datetime(2017, 12, 3, 8, 0, 0), SU_NIGHT),\n",
    "                             (datetime(2017, 12, 3, 8, 0, 0), datetime(2017, 12, 3, 21, 0, 0), SU_WEND),\n",
    "                             (datetime(2017, 12, 3, 21, 0, 0), datetime(2017, 12, 4, 8, 0, 0), SU_NIGHT),\n",
    "                             (datetime(2017, 12, 4, 8, 0, 0), datetime(2017, 12, 4, 21, 0, 0), SU_WDAY),\n",
    "                             (datetime(2017, 12, 4, 21, 0, 0), datetime(2017, 12, 5, 8, 0, 0), SU_WDAY)]\n",
    "    else:\n",
    "        assert False, \"Unknown scenario provided: \" + str(scenario)\n",
    "    for ts_start, ts_end, ts_subscen in INCLUDE_INTERVALS:\n",
    "        if ts_start < dt_tstamp <= ts_end:\n",
    "            return ts_subscen == subscenario\n",
    "    assert False, \"No matching timeframe found. dt=\" + str(dt)\n",
    "\n",
    "\n",
    "def is_colocated(sensor1, sensor2, scenario, dt=None):\n",
    "    \"\"\"Helper function to determine if two sensors are colocated in a specific scenario and time.\n",
    "    \n",
    "    :param sensor1: The first sensor number, as int\n",
    "    :param sensor2: The second sensor number, as int\n",
    "    :param scenario: The Scenario to consider\n",
    "    :param dt: A timestamp as string for the time to consider. Only required for mobile scenario\n",
    "    :return: true if devices are colocated, otherwise false.\"\"\"\n",
    "    # Car and office are simple lookups in their colocation tables\n",
    "    if scenario == S_CAR:\n",
    "        return COLO_CAR[sensor1-1][sensor2-1] == 1\n",
    "    if scenario == S_OFFICE:\n",
    "        return COLO_OFFICE[sensor1-1][sensor2-1] == 1\n",
    "    \n",
    "    # Mobile scenario is more involved due to mobile sensors\n",
    "    if scenario == S_MOBILE:\n",
    "        # Ensure a dt was provided\n",
    "        assert dt is not None\n",
    "        # Prepare state for locations of the three sensors\n",
    "        s1_loc = -1\n",
    "        s2_loc = -1\n",
    "        \n",
    "        # Lookup location of sensor 1 at the provided time\n",
    "        for ts_start, ts_end, ts_loc in COLO_MOBILE[sensor1]:\n",
    "            if ts_start <= dt < ts_end:\n",
    "                s1_loc = ts_loc\n",
    "                break\n",
    "        \n",
    "        # dito for sensor 2\n",
    "        for ts_start, ts_end, ts_loc in COLO_MOBILE[sensor2]:\n",
    "            if ts_start <= dt < ts_end:\n",
    "                s2_loc = ts_loc\n",
    "                break\n",
    "        \n",
    "        # Ensure we have sane locations for both\n",
    "        assert s1_loc != -1\n",
    "        assert s2_loc != -1\n",
    "        # If both are in same location => colocated. Return\n",
    "        return s1_loc == s2_loc\n",
    "    # Something is wrong, an unknown scenario identifier was provided\n",
    "    assert False, \"Unknown scenario provided: \" + scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the actual data from the disk and putting it into the right format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all for feature and params\n",
    "def load_audio_feature(feature, interval, scenario, subscenario=None, strip=None):\n",
    "    \"\"\"Load an audio feature from the disk and return it as a dictionary.\n",
    "    \n",
    "    :param feature: The feature (e.g., SOUNDPROOF, AUDIO_FP, ...) to load\n",
    "    :param interval: The interval (10 seconds, 30 seconds, ...) to load\n",
    "    :param scenario: The scenario (Car or Office) to load\n",
    "    :param subscenario: The subscenario to load, or None to load the full dataset.\n",
    "    :param strip: If set, strip all keys except this one from the results (to save memory). When providing a string,\n",
    "        it will be used as a key. When providing a List of strings, all strings will be used as keys.\n",
    "    :return: The data as a dict\n",
    "    \"\"\"\n",
    "    # Ensure that the scenario is valid\n",
    "    assert scenario in [S_CAR, S_OFFICE, S_MOBILE]\n",
    "    # Logic for car scenario\n",
    "    if scenario != S_OFFICE:\n",
    "        rv = {}\n",
    "        # Set upper and lower bounds for sensor numbers\n",
    "        if scenario == S_CAR:\n",
    "            lower_bound = 1\n",
    "            upper_bound = 12\n",
    "        elif scenario == S_MOBILE:\n",
    "            lower_bound = 2\n",
    "            upper_bound = 25\n",
    "        # Go through all sensors in the experiment\n",
    "        # sensors = [2, 3, 4, 11, 12, 13, 14, 18, 19, 20, 21]\n",
    "        # for i in range(len(sensors)):\n",
    "        for s1 in range(lower_bound, upper_bound + 1, 1):\n",
    "            # s1 = sensors[i]\n",
    "            rv[s1] = {}\n",
    "            # Go through all pairs this sensor is involved in as the left-hand sensor\n",
    "            for s2 in range(s1 + 1, upper_bound + 1, 1):\n",
    "            # for j in range(i+1, len(sensors)):\n",
    "                # s2 = sensors[j]\n",
    "                # Generate the path where we expect the result file to be\n",
    "                path = generate_path(scenario, \"audio/\", feature, interval, s1, s2)\n",
    "                \n",
    "                # If we are not supposed to strip values from the results, simply load the data\n",
    "                if strip is None:\n",
    "                    # We have not yet implemented loading subscenarios without stripping\n",
    "                    assert subscenario is None, \"Unsupported combination of parameters\"\n",
    "                    \n",
    "                    # Load the results\n",
    "                    results = load_file(path)\n",
    "                    for k in results.keys():\n",
    "                        ts = parser.parse(k)\n",
    "                        rv[s1][s2][ts] = results[k]\n",
    "                    # rv[s1][s2] = load_file(path)\n",
    "                    \n",
    "                # We should strip data from the results.\n",
    "                else:\n",
    "                    # Load data\n",
    "                    results = load_file(path)\n",
    "                    # Prepare dictionary\n",
    "                    rv[s1][s2] = {}\n",
    "                    \n",
    "                    # For all keys, only include them if they are wanted\n",
    "                    for k in results.keys():\n",
    "                        ts = parser.parse(k)\n",
    "                        # If the key is not in the desired subscenario, skip it entirely\n",
    "                        if not is_subscenario(scenario, subscenario, ts):\n",
    "                            continue\n",
    "                        \n",
    "                        # If we have a list of desired keys, include all these keys\n",
    "                        if type(strip) is list:\n",
    "                            rv[s1][s2][ts] = {s: results[k][s] for s in strip}\n",
    "                        # Otherwise, we only include the \"strip\" key.\n",
    "                        else:\n",
    "                            rv[s1][s2][ts] = {strip: results[k][strip]}\n",
    "        # Return the result\n",
    "        return rv\n",
    "    # Logic for office scenario (different folder structure requires different code)\n",
    "    else:\n",
    "        rv = {}\n",
    "        # Once again, go through all combinations of sensors\n",
    "        for s1 in range(1, 25, 1):\n",
    "            rv[s1] = {}\n",
    "            for s2 in range(s1 + 1, 25, 1):\n",
    "                rv[s1][s2] = {}\n",
    "                \n",
    "                # Generate one path to load for every day of the office experiment\n",
    "                for day in [\"audio/1_0-24h/\", \"audio/2_24-48h/\", \"audio/3_48-72h/\", \"audio/4_72-96h/\", \"audio/5_96-120h/\", \"audio/6_120-144h/\", \"audio/7_144-168h/\"]:\n",
    "                    path = generate_path(scenario, \"audio/\", feature, interval, s1, s2, day=day)\n",
    "                    \n",
    "                    # If we're not stripping the results, simply load the data\n",
    "                    if strip is None:\n",
    "                        assert subscenario is None, \"Unsupported combination of parameters\"\n",
    "                        rv[s1][s2][day] = load_file(path)\n",
    "                    \n",
    "                    # Otherwise, strip again and respect subscenarios (see above)\n",
    "                    else:\n",
    "                        results = load_file(path)\n",
    "                        rv[s1][s2][day] = {}\n",
    "                        for k in results.keys():\n",
    "                            if not is_subscenario(scenario, subscenario, k):\n",
    "                                continue\n",
    "                            if type(strip) is list:\n",
    "                                rv[s1][s2][day][k] = {s: results[k][s] for s in strip}\n",
    "                            else:\n",
    "                                rv[s1][s2][day][k] = {strip: results[k][strip]}\n",
    "        # Return the assembled results\n",
    "        return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used to generate False Accept Rates (FARs) and False Reject Rates (FRRs) for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate False Accept and False Reject Rate\n",
    "def gen_far_frr(scenario, data, param, minv=None, maxv=None, increments=1000, filter_func=None, threshold=None):\n",
    "    \"\"\"Generate a bunch of statistics on the sensor pairs to allow calculation of error rates, overlap, etc.\n",
    "    \n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param data: The data to operate on, as loaded by load_audio_feature\n",
    "    :param param: The parameter to operate on (e.g., max_xcorr)\n",
    "    :param minv: The minimum value to use for the threshold search\n",
    "    :param maxv: The maximum value to use for the threshold search\n",
    "    :param increments: The number of increments the area between minv and maxv should be divided\n",
    "    :param filter_func: A function to apply to all data before further processing, to exclude specific samples\n",
    "        that do not meet some criteria (e.g., energy too low to consider, ...)\n",
    "    :param threshold: Use a fixed threshold for computations instead of finding your own between minv and maxv.\n",
    "    :return: A dictionary containing false positive, false negative, true positive and true negative counts for\n",
    "        each considered threshold and pair of sensors.\"\"\"\n",
    "    # Histogram intersection function adapted from\n",
    "    # http://blog.datadive.net/histogram-intersection-for-change-detection/\n",
    "    def histogram_intersection(h1, h2, bins):\n",
    "        bins = np.diff(bins)\n",
    "        sm = 0\n",
    "        for i in range(len(bins)):\n",
    "            sm += min(bins[i]*h1[i], bins[i]*h2[i])\n",
    "        return sm\n",
    "    \n",
    "    # Ensure the scenario is sane\n",
    "    assert scenario in [S_CAR, S_OFFICE, S_MOBILE]\n",
    "    # Sanity check that the provided data makes sense\n",
    "    if (minv is not None and maxv is None) or (minv is None and maxv is not None):\n",
    "        raise Exception(\"Need to provide either both or none of minv, maxv\")\n",
    "    if (minv is not None and maxv is not None and threshold is not None):\n",
    "        raise Exception(\"Do not provide both a fixed threshold and an area for a threshold search.\")\n",
    "    \n",
    "    # Prepare a few variables\n",
    "    filter_res = None\n",
    "    colo = []\n",
    "    ncolo = []\n",
    "    \n",
    "    # Apply filter function. This allows us to enforce certain requirements on the data\n",
    "    # (e.g., minimum energy, ...)\n",
    "    if filter_func is not None:\n",
    "        data, filter_res = filter_func(data, scenario)\n",
    "        print(\"Filter result:\")\n",
    "        pprint(filter_res)\n",
    "    res = {}\n",
    "    \n",
    "    # Find the range (minimum and maximum values) to try for the threshold, based on the data\n",
    "    if (minv is None or maxv is None) or threshold is not None:\n",
    "        #print(\"Finding min and max...\")\n",
    "        minv = 100000.0\n",
    "        maxv = 0.0\n",
    "        if scenario == S_CAR or scenario == S_MOBILE:\n",
    "            for s1 in data:\n",
    "                for s2 in data[s1]:\n",
    "                    for d in data[s1][s2]:\n",
    "                        minv = min(data[s1][s2][d][param], minv)\n",
    "                        maxv = max(data[s1][s2][d][param], maxv)\n",
    "        if scenario == S_OFFICE:\n",
    "            for s1 in data:\n",
    "                for s2 in data[s1]:\n",
    "                    for day in data[s1][s2]:\n",
    "                        for d in data[s1][s2][day]:\n",
    "                            minv = min(data[s1][s2][day][d][param], minv)\n",
    "                            maxv = max(data[s1][s2][day][d][param], maxv)\n",
    "        print(\"min: %s, max: %s\" % (minv, maxv))\n",
    "        #print(\"Threshold fpr, fnr, tpr, tnr\")\n",
    "        \n",
    "        if 0.0 <= minv <= 1.0 and 0.0 <= maxv <= 1.0:\n",
    "            minv = 0.0\n",
    "            maxv = 1.0\n",
    "        elif 0.0 <= minv <= 100.0 and 0.0 <= maxv <= 100.0:\n",
    "            minv = 0.0\n",
    "            maxv = 100.0\n",
    "        else:\n",
    "            raise Exception(\"Not implemented\")\n",
    "\n",
    "    # Determine error counts for all considered thresholds\n",
    "    if threshold is None:\n",
    "        # Try the requested number of increments between minv and maxv\n",
    "        for i in range(increments+1):\n",
    "            # Calculate current threshold for this increment\n",
    "            threshold = minv + i * ((maxv - minv) / increments)\n",
    "            \n",
    "            res[threshold] = {}\n",
    "            \n",
    "            # If we are looking at data from the car scenario...\n",
    "            if scenario == S_CAR or scenario == S_MOBILE:\n",
    "                # ... iterate through all pairs of sensors\n",
    "                for s1 in data:\n",
    "                    res[threshold][s1] = {}\n",
    "                    for s2 in data[s1]:\n",
    "                        res[threshold][s1][s2] = {\n",
    "                            \"ta\": 0.0,  # True accept\n",
    "                            \"tr\": 0.0,  # True reject\n",
    "                            \"fa\": 0.0,  # False accept\n",
    "                            \"fr\": 0.0   # False reject\n",
    "                        }\n",
    "                        # Iterate through all readings for this pair\n",
    "                        for d in data[s1][s2]:\n",
    "                            # If the reading is above the threshold, accept it, otherwise reject\n",
    "                            accept = data[s1][s2][d][param] >= threshold\n",
    "                            # Check if we accepted correctly or incorrectly and track for error rates\n",
    "                            if is_colocated(s1, s2, scenario, d):\n",
    "                                if i == 0:\n",
    "                                    colo.append(data[s1][s2][d][param])\n",
    "                                if accept:\n",
    "                                    res[threshold][s1][s2][\"ta\"] += 1\n",
    "                                    # true_acc += 1\n",
    "                                else:\n",
    "                                    res[threshold][s1][s2][\"fr\"] += 1\n",
    "                                    # false_rej += 1\n",
    "                            else:\n",
    "                                if i == 0:\n",
    "                                    ncolo.append(data[s1][s2][d][param])\n",
    "                                if accept:\n",
    "                                    res[threshold][s1][s2][\"fa\"] += 1\n",
    "                                    # false_acc += 1\n",
    "                                else:\n",
    "                                    res[threshold][s1][s2][\"tr\"] += 1\n",
    "                                    # true_rej += 1\n",
    "            \n",
    "            # Do the same for the office (slightly more complicated due to different data structure)\n",
    "            if scenario == S_OFFICE:\n",
    "                for s1 in data:\n",
    "                    res[threshold][s1] = {}\n",
    "                    for s2 in data[s1]:\n",
    "                        res[threshold][s1][s2] = {\n",
    "                            \"ta\": 0.0,  # True accept\n",
    "                            \"tr\": 0.0,  # True reject\n",
    "                            \"fa\": 0.0,  # False accept\n",
    "                            \"fr\": 0.0   # False reject\n",
    "                        }\n",
    "                        for day in data[s1][s2]:\n",
    "                            for d in data[s1][s2][day]:\n",
    "                                accept = data[s1][s2][day][d][param] >= threshold\n",
    "                                if is_colocated(s1, s2, scenario):\n",
    "                                    if i == 0:\n",
    "                                        colo.append(data[s1][s2][day][d][param])\n",
    "                                    if accept:\n",
    "                                        res[threshold][s1][s2][\"ta\"] += 1\n",
    "                                        # true_acc += 1\n",
    "                                    else:\n",
    "                                        res[threshold][s1][s2][\"fr\"] += 1\n",
    "                                        # false_rej += 1\n",
    "                                else:\n",
    "                                    if i == 0:\n",
    "                                        ncolo.append(data[s1][s2][day][d][param])\n",
    "                                    if accept:\n",
    "                                        res[threshold][s1][s2][\"fa\"] += 1\n",
    "                                        # false_acc += 1\n",
    "                                    else:\n",
    "                                        res[threshold][s1][s2][\"tr\"] += 1\n",
    "                                        # true_rej += 1\n",
    "    \n",
    "    else:\n",
    "        # A fixed threshold has been given. Only check this threshold\n",
    "        true_acc = 0.0\n",
    "        false_acc = 0.0\n",
    "        true_rej = 0.0\n",
    "        false_rej = 0.0\n",
    "    \n",
    "        # Go through everything as before (see above), but this time do not generate\n",
    "        # a result dictionary. Instead, we just want to compute the error rates for\n",
    "        # the specified threshold and print them.\n",
    "        if scenario == S_CAR or scenario == S_MOBILE:\n",
    "            for s1 in data:\n",
    "                for s2 in data[s1]:\n",
    "                    for d in data[s1][s2]:\n",
    "                        accept = data[s1][s2][d][param] >= threshold\n",
    "                        if is_colocated(s1, s2, scenario, d):\n",
    "                            if accept:\n",
    "                                true_acc += 1\n",
    "                            else:\n",
    "                                false_rej += 1\n",
    "                        else:\n",
    "                            if accept:\n",
    "                                false_acc += 1\n",
    "                            else:\n",
    "                                true_rej += 1\n",
    "        if scenario == S_OFFICE:\n",
    "            for s1 in data:\n",
    "                for s2 in data[s1]:\n",
    "                    for day in data[s1][s2]:\n",
    "                        for d in data[s1][s2][day]:\n",
    "                            accept = data[s1][s2][day][d][param] >= threshold\n",
    "                            if is_colocated(s1, s2, scenario):\n",
    "                                if accept:\n",
    "                                    true_acc += 1\n",
    "                                else:\n",
    "                                    false_rej += 1\n",
    "                            else:\n",
    "                                if accept:\n",
    "                                    false_acc += 1\n",
    "                                else:\n",
    "                                    true_rej += 1\n",
    "\n",
    "        # Calculate error rates\n",
    "        fpr = false_acc / (false_acc + true_rej)\n",
    "        fnr = false_rej / (false_rej + true_acc)\n",
    "        tpr = true_acc / (true_acc + false_rej)\n",
    "        tnr = true_rej / (true_rej + false_acc)\n",
    "        #print(threshold, fpr, fnr, tpr, tnr)\n",
    "        # print(\"far\", fpr, \"frr\", fnr)\n",
    "        return {\"far\": fpr, \"frr\": fnr, \"threshold\": threshold}\n",
    "    \n",
    "    # Plot the overlap between the classes\n",
    "    plot_distributions(data, scenario, param, minv, maxv)\n",
    "    \n",
    "    # Also calculate overlap as a number\n",
    "    hc, bins = np.histogram(colo, np.arange(minv, maxv, maxv/100.0), density=True)\n",
    "    hn, _ = np.histogram(ncolo, np.arange(minv, maxv, maxv/100.0), density=True)\n",
    "    print(\"Intersection:\", histogram_intersection(hc, hn, bins))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def frr_for_far(data, target_far, sources=None, targets=None):\n",
    "    \"\"\"Calculate the False Reject Rate (FRR) implied by a given target False Accept Rate (FAR).\n",
    "    Can also compute this for subsets of the datasets, given by \"sources\" and \"targets\". In this\n",
    "    case, it will consider all combinations of sources and targets (e.g., if sources = [\"1\"] and\n",
    "    targets = [\"2\", \"3\"], it will consider 1-2 and 1-3, but not 2-3).\n",
    "    \n",
    "    :param data: The data as a dictionary, as produced by gen_far_frr or the import functions.\n",
    "    :param target_far: The false accept rate to aim for. Note that 1.0 implies 100%, so 0.1% should\n",
    "        be written as 0.001.\n",
    "    :param sources: A list of sensors (as unpadded strings) to use as sources, or None if all\n",
    "        should be considered.\n",
    "    :param targets: A list of sensors (as unpadded strings) to use as targets, or NOne if all\n",
    "        should be considered\n",
    "    :return: A 3-tuple of observed FAR, FRR, and the used threshold.\n",
    "    \"\"\"\n",
    "    def is_source(sensor):\n",
    "        if sources is None:\n",
    "            return True\n",
    "        else:\n",
    "            return str(sensor) in sources\n",
    "    \n",
    "    def is_target(sensor):\n",
    "        if targets is None:\n",
    "            return True\n",
    "        else:\n",
    "            return str(sensor) in targets\n",
    "\n",
    "    # Initialize previous values with bogus values to ensure they are never used in the first iteration.\n",
    "    prev_far = -500.0\n",
    "    prev_frr = -500.0\n",
    "    prev_threshold = -500.0\n",
    "    for threshold in data:\n",
    "        false_acc = 0.0\n",
    "        false_rej = 0.0\n",
    "        true_acc = 0.0\n",
    "        true_rej = 0.0\n",
    "        for s1 in data[threshold]:\n",
    "            # Check if sensor is supposed to be included\n",
    "            if not (is_source(s1) or is_target(s1)):\n",
    "                continue\n",
    "            for s2 in data[threshold][s1]:\n",
    "                # Check if sensor pairing is supposed to be included\n",
    "                if not ((is_source(s1) and is_target(s2)) or (is_source(s2) and is_target(s1))):\n",
    "                    continue\n",
    "                # Include the numbers in the overall count\n",
    "                false_acc += data[threshold][s1][s2]['fa']\n",
    "                false_rej += data[threshold][s1][s2]['fr']\n",
    "                true_acc += data[threshold][s1][s2]['ta']\n",
    "                true_rej += data[threshold][s1][s2]['tr']\n",
    "        \n",
    "        # Calculate error rates\n",
    "        far = false_acc / (false_acc + true_rej)\n",
    "        frr = false_rej / (false_rej + true_acc)\n",
    "        \n",
    "        if far > target_far:\n",
    "            # The computed FAR is above the target FAR. Save current values and carry on\n",
    "            prev_far = far\n",
    "            prev_frr = frr\n",
    "            prev_threshold = threshold\n",
    "        else:\n",
    "            # We have reached or passed the target FAR. Determine if the previous value was a better fit\n",
    "            if abs(target_far - far) < abs(target_far - prev_far):\n",
    "                # We are closer to the target FAR than the previous FAR, use our values\n",
    "                return (far, frr, threshold)\n",
    "            else:\n",
    "                # The previous values were closer, use them\n",
    "                return (prev_far, prev_frr, prev_threshold)\n",
    "    assert False, \"This statement should never be reached. Last error rates: FAR \" + str(prev_far) + \", FRR \" + str(prev_frr)\n",
    "\n",
    "    \n",
    "def error_rate_for_threshold(data, threshold):\n",
    "    \"\"\"Calculate the error rates when using a specific threshold.\n",
    "    \n",
    "    :param data: The data, as generated by gen_far_frr\n",
    "    :param threshold: The threshold to generate the error rates for\n",
    "    :return: A 2-tuple of far, frr\n",
    "    \"\"\"\n",
    "    false_acc = 0.0\n",
    "    false_rej = 0.0\n",
    "    true_acc = 0.0\n",
    "    true_rej = 0.0\n",
    "    for s1 in data[threshold]:\n",
    "        for s2 in data[threshold][s1]:\n",
    "            # Include the numbers in the overall count\n",
    "            false_acc += data[threshold][s1][s2]['fa']\n",
    "            false_rej += data[threshold][s1][s2]['fr']\n",
    "            true_acc += data[threshold][s1][s2]['ta']\n",
    "            true_rej += data[threshold][s1][s2]['tr']\n",
    "\n",
    "    # Calculate error rates\n",
    "    far = false_acc / (false_acc + true_rej)\n",
    "    frr = false_rej / (false_rej + true_acc)\n",
    "        \n",
    "    return (far, frr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for saving and loading results, and their helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting JSON data from the get_far_frr function to a file\n",
    "def save_result_json(data, paper, interval, scenario, modality, subscenario=None, suffix=None):\n",
    "    \"\"\"Save a dictionary into a file as JSON. Mostly used for cache files.\n",
    "    \n",
    "    :param data: The data to save.\n",
    "    :param paper: The paper to save it under (SOUNDPROOF, ...)\n",
    "    :param interval: The interval to save it under\n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param modality: The modality (max_xcorr, ...)\n",
    "    :param subscenario: The subscenario, or None if global.\n",
    "    :param suffix: A suffix to place before the .json\"\"\"\n",
    "    path = '/'.join([PREFIX_JSON, scenario, paper, modality])\n",
    "    filename = path + '/' + interval\n",
    "    if subscenario is not None:\n",
    "        filename += '-' + subscenario\n",
    "    if suffix is not None:\n",
    "        filename += '_' + suffix\n",
    "    filename += '.json'\n",
    "    makedirs(path, exist_ok=True)\n",
    "    with open(filename, 'w') as fo:\n",
    "        json.dump(data, fo, separators=(',', ': '), indent=4)\n",
    "\n",
    "# Check if a result cache file (generated by save_result_json) exists\n",
    "def result_exists(paper, interval, scenario, modality, subscenario=None, suffix=None):\n",
    "    \"\"\"Check if a cache file for a set of parameters exists.\n",
    "    \n",
    "    :param paper: The paper (SOUNDPROOF, ...)\n",
    "    :param interval: The interval\n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param modality: The modality (max_xcorr, ...)\n",
    "    :param subscenario: The subscenario, or None if global.\n",
    "    :return: True if a file exists, otherwise False\"\"\"\n",
    "    path = '/'.join([PREFIX_JSON, scenario, paper, modality])\n",
    "    filename = path + '/' + interval\n",
    "    if subscenario is not None:\n",
    "        filename += '-' + subscenario\n",
    "    if suffix is not None:\n",
    "        filename += '_' + suffix\n",
    "    filename += '.json'\n",
    "    return isfile(filename)\n",
    "\n",
    "# Load result cache file (generated by save_result_json)\n",
    "def load_result(paper, interval, scenario, modality, subscenario=None, suffix=None):\n",
    "    \"\"\"Check if a cache file for a set of parameters exists.\n",
    "    \n",
    "    :param paper: The paper(SOUNDPROOF, ...)\n",
    "    :param interval: The interval\n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param modality: The modality (max_xcorr, ...)\n",
    "    :param subscenario: The subscenario, or None if global.\n",
    "    :return: The loaded cache as a dictionary\"\"\"\n",
    "    assert result_exists(paper, interval, scenario, modality, subscenario, suffix)\n",
    "    rv = {}\n",
    "    \n",
    "    # Construct file name\n",
    "    path = '/'.join([PREFIX_JSON, scenario, paper, modality])\n",
    "    filename = path + '/' + interval\n",
    "    if subscenario is not None:\n",
    "        filename += '-' + subscenario\n",
    "    if suffix is not None:\n",
    "        filename += '_' + suffix\n",
    "    filename += '.json'\n",
    "    \n",
    "    # Load the data\n",
    "    with open(filename, 'r') as fo:\n",
    "        r = json.load(fo)\n",
    "    \n",
    "    # Cast thresholds to float, if necessary\n",
    "    if suffix is None:\n",
    "        for threshold in r:\n",
    "            rv[float(threshold)] = r[threshold]\n",
    "        return rv\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FAR and FRR\n",
    "def plot_far_frr(result_data, paper=None, scenario=None, subscenario=None, modality=None, feature=None, interval=None, save=False, xlow=0.0, xhigh=1.0):\n",
    "    \"\"\"Generate the FAR and FRR for specific results data, and determine and plot the EER.\n",
    "    \n",
    "    :param result_data: data in the format generated by gen_far_frr or the import functions\n",
    "    :param xlow: lower limit for the threshold search\n",
    "    :param xhigh: upper limit for the threshold search\n",
    "    :return: A 3-tuple (threshold, fpr, fnr)\n",
    "    \"\"\"\n",
    "    # We are getting a fairly complex input, so we first have to generate a simpler data structure from it\n",
    "    if save:\n",
    "        assert paper is not None\n",
    "        assert scenario is not None\n",
    "        assert interval is not None\n",
    "        assert modality is not None\n",
    "        assert feature is not None\n",
    "        filename = '/'.join([PREFIX_PLOTS, scenario, paper, modality])\n",
    "        if subscenario is not None:\n",
    "            filename += '/' + '-'.join([feature, interval, subscenario, 'error-rates']) + '.eps'\n",
    "        else:\n",
    "            filename += '/' + '-'.join([feature, interval, 'error-rates']) + '.eps'\n",
    "    results = {}\n",
    "    for threshold in result_data:\n",
    "        # Prepare result and temporary variables\n",
    "        results[threshold] = {}\n",
    "        true_acc = 0.0\n",
    "        true_rej = 0.0\n",
    "        false_acc = 0.0\n",
    "        false_rej = 0.0\n",
    "        \n",
    "        # Load counts into the temporary vars\n",
    "        for s1 in result_data[threshold]:\n",
    "            for s2 in result_data[threshold][s1]:\n",
    "                true_acc += result_data[threshold][s1][s2][\"ta\"]\n",
    "                true_rej += result_data[threshold][s1][s2][\"tr\"]\n",
    "                false_acc += result_data[threshold][s1][s2][\"fa\"]\n",
    "                false_rej += result_data[threshold][s1][s2][\"fr\"]\n",
    "        \n",
    "        # Calculate error rates\n",
    "        # False Accept Rate (FAR)\n",
    "        fpr = false_acc / (false_acc + true_rej)\n",
    "        # False Reject Rate (FRR)\n",
    "        fnr = false_rej / (false_rej + true_acc)\n",
    "        # True Accept Rate (TAR)\n",
    "        tpr = true_acc / (true_acc + false_rej)\n",
    "        # True Reject Rate (TRR)\n",
    "        tnr = true_rej / (true_rej + false_acc)\n",
    "\n",
    "        # Put them in a data structure the visualization function understands\n",
    "        results[threshold] = {\"fpr\": fpr, \"fnr\": fnr, \"tpr\": tpr, \"tnr\": tnr}\n",
    "    \n",
    "    # Actually do the plotting of the error rates\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(sorted(results.keys()), [results[threshold][\"fpr\"] for threshold in sorted(results.keys())], '--', label='FAR')\n",
    "    ax.plot(sorted(results.keys()), [results[threshold][\"fnr\"] for threshold in sorted(results.keys())], label='FRR')\n",
    "    \n",
    "    # Determine the point where FAR and FRR are closest (i.e., the Equal Error Rate - EER)\n",
    "    for threshold in sorted(results.keys()):\n",
    "        if results[threshold][\"fpr\"] <= results[threshold][\"fnr\"]:\n",
    "            fpr = results[threshold][\"fpr\"]\n",
    "            fnr = results[threshold][\"fnr\"]\n",
    "            if abs(prev_fpr - prev_fnr) < abs(results[threshold][\"fpr\"] - results[threshold][\"fnr\"]):\n",
    "                fpr = prev_fpr\n",
    "                fnr = prev_fnr\n",
    "                threshold = prev_thres\n",
    "            print(\"Thresh\", threshold, \"FAR\", fpr, \"FRR\", fnr)\n",
    "            #ax.plot([threshold, threshold], [0.0, fnr], 'k-')\n",
    "            #ax.plot([0.0, threshold], [fnr, fnr], 'k-')\n",
    "            ax.set(xlabel='Similarity Threshold', ylabel='Error Rate', xlim=(xlow,xhigh))\n",
    "            ax.legend()\n",
    "            if save:\n",
    "                plt.savefig(filename, format='eps', dpi=1000)\n",
    "            plt.show()\n",
    "\n",
    "            return fpr, fnr, threshold\n",
    "        prev_fpr = results[threshold][\"fpr\"]\n",
    "        prev_fnr = results[threshold][\"fnr\"]\n",
    "        prev_thres = threshold\n",
    "\n",
    "\n",
    "def prune_results(result_data, prune):\n",
    "    \"\"\"Remove specific sensors from the results\n",
    "    \n",
    "    :param result_data: Result data, as formatted by gen_far_frr or the result import functions\n",
    "    :param prune: A list of dictionary keys (strings) to remove. Will remove any pair that involves at least\n",
    "        one of the provided keys (so if \"01\" should be removed, it will remove X->01 and 01->X for all keys X)\n",
    "    :return: the pruned result dataset\n",
    "    \"\"\"\n",
    "    # Make a deep copy of the dictionary, because we don't want to modify the original\n",
    "    result = copy.deepcopy(result_data)\n",
    "    # Go through the dictionary looking for keys to remove\n",
    "    for t in result:\n",
    "        # Ensure that we are not using an internal iterator on the dictionary, since we are about to modify the\n",
    "        # dictionary during iteration\n",
    "        for s1 in list(result[t].keys()):\n",
    "            if s1 in prune:\n",
    "                del(result[t][s1])\n",
    "            else:\n",
    "                # Ditto here\n",
    "                for s2 in list(result[t][s1]):\n",
    "                    if s2 in prune:\n",
    "                        del([result[t][s1][s2]])\n",
    "    return result\n",
    "\n",
    "\n",
    "def pair_error_rate(result_data, s1, s2, threshold):\n",
    "    \"\"\"Determine the error rate between sensors s1 and s2.\n",
    "    \n",
    "    :param result_data: Result data, as formatted by gen_far_frr or the result import functions\n",
    "    :param s1: The first sensor (string)\n",
    "    :param s2: The second sensor (string)\n",
    "    :param threshold: The threshold to use\n",
    "    :return: 4-tuple of FAR, FRR, TAR, TRR.\n",
    "    \"\"\"\n",
    "    # Ensure dataset meets expectations\n",
    "    assert threshold in result_data\n",
    "    if int(s1) > int(s2):\n",
    "        t = s1\n",
    "        s1 = s2\n",
    "        s2 = t\n",
    "    assert s1 in result_data[threshold]\n",
    "    assert s2 in result_data[threshold][s1]\n",
    "    \n",
    "    # Collect statistics\n",
    "    true_acc = result_data[threshold][s1][s2][\"ta\"]\n",
    "    true_rej = result_data[threshold][s1][s2][\"tr\"]\n",
    "    false_acc = result_data[threshold][s1][s2][\"fa\"]\n",
    "    false_rej = result_data[threshold][s1][s2][\"fr\"]\n",
    "    \n",
    "    # Calculate error rates\n",
    "    # False Accept Rate (FAR)\n",
    "    if false_acc + true_rej > 0:\n",
    "        fpr = false_acc / (false_acc + true_rej)\n",
    "    else:\n",
    "        fpr = None\n",
    "\n",
    "    # False Reject Rate (FRR)\n",
    "    if false_rej + true_acc > 0:\n",
    "        fnr = false_rej / (false_rej + true_acc)\n",
    "    else:\n",
    "        fnr = None\n",
    "\n",
    "    # True Accept Rate (TAR)\n",
    "    if true_acc + false_rej > 0:\n",
    "        tpr = true_acc / (true_acc + false_rej)\n",
    "    else:\n",
    "        tpr = None\n",
    "    \n",
    "    # True Reject Rate (TRR)\n",
    "    if true_rej + false_acc > 0:\n",
    "        tnr = true_rej / (true_rej + false_acc)\n",
    "    else:\n",
    "        tnr = None\n",
    "    \n",
    "    return (fpr, fnr, tpr, tnr)\n",
    "    \n",
    "    \n",
    "def plot_distributions(data, scenario, param, minv, maxv, legend=True, save=None):\n",
    "    \"\"\"Plot the distribution of values to see how far they overlap.\n",
    "    \n",
    "    :param data: The data to plot.\n",
    "    :param scenario: The scenario (S_CAR, S_OFFICE)\n",
    "    :param param: The parameter to look into (max_xcorrm ...)\n",
    "    :param minv: The minimum on the x-axis to show\n",
    "    :param maxv: The maximum on the x-axis to show\n",
    "    :param legend: Boolean indicating whether a legend should be printed\n",
    "    :param save: A path under which the plot should be saved, or None if it should not be saved.\"\"\"\n",
    "    colo = []\n",
    "    ncolo = []\n",
    "    if scenario == S_CAR or scenario == S_MOBILE:\n",
    "        for s1 in data:\n",
    "            for s2 in data[s1]:\n",
    "                for d in data[s1][s2]:\n",
    "                    if is_colocated(s1, s2, scenario, d):\n",
    "                        colo.append(data[s1][s2][d][param])\n",
    "                    else:\n",
    "                        ncolo.append(data[s1][s2][d][param])\n",
    "    else:\n",
    "        for s1 in data:\n",
    "            for s2 in data[s1]:\n",
    "                if is_colocated(s1, s2, scenario):\n",
    "                    for day in data[s1][s2]:\n",
    "                        for d in data[s1][s2][day]:\n",
    "                            if not math.isnan(data[s1][s2][day][d][param]):\n",
    "                                colo.append(data[s1][s2][day][d][param])\n",
    "                else:\n",
    "                    for day in data[s1][s2]:\n",
    "                        for d in data[s1][s2][day]:\n",
    "                            if not math.isnan(data[s1][s2][day][d][param]):\n",
    "                                ncolo.append(data[s1][s2][day][d][param])\n",
    "    plt.figure()\n",
    "    sns.kdeplot(colo, label=\"Colocated\")\n",
    "    ax = sns.kdeplot(ncolo, label=\"Non-colocated\", linestyle=\"--\")\n",
    "    # ax.set_xlabel(\"Similarity Percentage\", fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    if legend:\n",
    "        plt.legend(prop={'size': 14})\n",
    "    if not legend:\n",
    "        ax.legend().set_visible(False)\n",
    "    if 0.0 <= minv <= 1.0 and 0.0 <= maxv <= 1.0:\n",
    "        minlim = 0.0\n",
    "        maxlim = 1.0\n",
    "    elif 0.0 <= minv <= 100.0 and 0.0 <= maxv <= 100.0:\n",
    "        minlim = 0.0\n",
    "        maxlim = 100.0\n",
    "    else:\n",
    "        raise Exception(\"Not implemented\")\n",
    "    ax.set_xlim(minlim,maxlim)\n",
    "    if save is not None:\n",
    "        plt.savefig(save, format='eps', dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "At this point, we have defined all functions that we need for data analysis.\n",
    "\n",
    "To work with the FAR and FRR data, we need to first compute a series of accept and reject results for specific thresholds on the data. **This is an expensive operation** (on the order of up to a few days for large datasets like the office experiment), but it only needs to be run once - the results are cached on disk for future operations.\n",
    "\n",
    "If you have obtained this code together with the dataset, it should already contain these caches, no need to regenerate them unless you want to reproduce our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_LIST = {\n",
    "    S_CAR: [SU_PARKED, SU_CITY, SU_HIGHWAY, None],\n",
    "    S_OFFICE: [SU_WDAY, SU_NIGHT, SU_WEND, None],\n",
    "    S_MOBILE: [None]\n",
    "}\n",
    "for scenario in [S_CAR, S_OFFICE, S_MOBILE]:\n",
    "    for interval in reversed(INTERVALS):\n",
    "        for subscenario in SUB_LIST[scenario]:\n",
    "            print(scenario, subscenario, interval)\n",
    "            # Load the data\n",
    "            data = load_audio_feature(AUDIO_FP, interval, scenario, strip='fingerprints_similarity_percent', subscenario=subscenario)\n",
    "            # Compute the FAR and FRR for a set of different thresholds\n",
    "            rv = gen_far_frr(scenario, data, 'fingerprints_similarity_percent', minv=0.0, maxv=100.0, increments=3000)\n",
    "            # Save the resulting caches\n",
    "            save_result_json(rv, AUDIO_FP, interval, scenario, 'fingerprints_similarity_percent', subscenario=subscenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the caches, we can generate the actual results. This includes the actual error rates, the Equal Error Rates (EERs), and a few more statistics on all scenarios and subscenarios with all interval sizes. It also computes the robustness data (i.e, applying the EER threshold from one scenario on another dataset and seeing what the resulting error rates are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIO_SET = set([S_CAR, S_OFFICE, S_MOBILE])\n",
    "SUB_LIST = {\n",
    "    S_CAR: [SU_PARKED, SU_CITY, SU_HIGHWAY, None],\n",
    "    S_OFFICE: [SU_WDAY, SU_NIGHT, SU_WEND, None],\n",
    "    S_MOBILE: [None]\n",
    "}\n",
    "\n",
    "\n",
    "SUB_SETS = {\n",
    "    S_CAR: set([SU_PARKED, SU_CITY, SU_HIGHWAY]),\n",
    "    S_OFFICE: set([SU_WDAY, SU_NIGHT, SU_WEND]),\n",
    "    S_MOBILE: set([None])\n",
    "}\n",
    "\n",
    "robustness_output = {}\n",
    "\n",
    "# We go through every interval...\n",
    "for interval in reversed(INTERVALS):\n",
    "    thresholds = {}\n",
    "    # ...and every scenario\n",
    "    for scenario in SCENARIO_SET:\n",
    "        thresholds[scenario] = {}\n",
    "        # ...and every subscenario (including \"None\", which is the full scenario)\n",
    "        for subscenario in SUB_LIST[scenario]:\n",
    "            thresholds[scenario][subscenario] = {}\n",
    "            print(scenario, subscenario, interval)\n",
    "            # Prepare output JSON\n",
    "            result = {\"base\": {}, \"adversarial\": {}}\n",
    "\n",
    "            # We have cached results, load them\n",
    "            print(\"Using cached results...\")\n",
    "            rv = load_result(AUDIO_FP, interval, scenario, 'fingerprints_similarity_percent', subscenario=subscenario)\n",
    "            \n",
    "            # Plot the error rates and determine the EER\n",
    "            far, frr, thresh = plot_far_frr(rv, xlow=0.0, xhigh=100.0, paper=AUDIO_FP, scenario=scenario, subscenario=subscenario, modality='audio', feature='fingerprints_similarity_percent', interval=interval)\n",
    "            result[\"base\"][\"eer\"] = {\"far\": far, \"frr\": frr, \"threshold\": thresh}\n",
    "            thresholds[scenario][subscenario][\"eer\"] = {\"threshold\": thresh, \"far\": far, \"frr\": frr}\n",
    "            \n",
    "            # Try a number of target false accept rates to determine the respective FRR and threshold\n",
    "            for target_far in [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05]:\n",
    "                far, frr, threshold = frr_for_far(rv, target_far)\n",
    "                result[\"base\"][\"far_%s\" % target_far] = {\"far\": far, \"frr\": frr, \"threshold\": threshold}\n",
    "                thresholds[scenario][subscenario][\"far_%s\" % target_far] = {\"threshold\": threshold, \"far\": far, \"frr\": frr}\n",
    "            \n",
    "            # Save the result\n",
    "            save_result_json(result, AUDIO_FP, interval, scenario, 'fingerprints_similarity_percent', subscenario=subscenario, suffix=\"rates\")\n",
    "            \n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "    # At this point, the thresholds dictionary should contain information for all scenarios and subscenarios\n",
    "    # We will now perform robustness checks.  This means that we are interested in how well the performance\n",
    "    # holds up if we start using the optimal threshold from one scenario on the other scenario, and the same\n",
    "    # for subscenarios within each scenario.  This is written less efficiently than it could be, in the interest\n",
    "    # of simpler code - loading all result caches twice does not have a massive impact on the performance.\n",
    "    print(\"Generating robustness data...\")\n",
    "    # Again, go through all scenarios and subscenarios\n",
    "    for scenario in SCENARIO_SET:\n",
    "        result = {scenario: {}}\n",
    "        for subscenario in SUB_LIST[scenario]:\n",
    "            \n",
    "            error_rates = [\"far_%s\" % rate for rate in [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05]]\n",
    "            error_rates.append(\"eer\")\n",
    "            \n",
    "            # We have cached results, load them\n",
    "            rv = load_result(AUDIO_FP, interval, scenario, 'fingerprints_similarity_percent', subscenario=subscenario)\n",
    "            \n",
    "            if subscenario is None:\n",
    "                # This is a \"global\" result (no specific subscenario) - compare with other scenario\n",
    "                for target in SCENARIO_SET - set([scenario]):\n",
    "                    # Prepare output\n",
    "                    result[scenario][target] = {}\n",
    "                    \n",
    "                    # Go through all target error rates (see above)\n",
    "                    for error_rate in error_rates:\n",
    "                        # Load the threshold from the target scenarios\n",
    "                        threshold = thresholds[target][None][error_rate][\"threshold\"]\n",
    "                        orig_far = thresholds[scenario][None][error_rate][\"far\"]\n",
    "                        orig_frr = thresholds[scenario][None][error_rate][\"frr\"]\n",
    "\n",
    "                        # Determine the error rate the scenario would have with the threshold from the target scenario\n",
    "                        far, frr = error_rate_for_threshold(rv, threshold)\n",
    "                        # ...and save it.\n",
    "                        result[scenario][target][error_rate] = {\n",
    "                            \"threshold\": threshold, \n",
    "                            \"far\": far, \n",
    "                            \"frr\": frr,\n",
    "                        }\n",
    "                        \n",
    "                        # We also want to print out a formatted robustness table, but only for the EER threshold\n",
    "                        if error_rate == \"eer\":\n",
    "                            # Save the old error rates\n",
    "                            result[scenario][target][error_rate][\"orig_far\"] = orig_far\n",
    "                            result[scenario][target][error_rate][\"orig_frr\"] = orig_frr\n",
    "                            \n",
    "                            # Ensure the structure of the robustness_output dictionary will support our actions\n",
    "                            if scenario not in robustness_output:\n",
    "                                robustness_output[scenario] = {target: {}}\n",
    "                            if target not in robustness_output[scenario]:\n",
    "                                robustness_output[scenario][target] = {}\n",
    "                                \n",
    "                            # Save the robustness information\n",
    "                            robustness_output[scenario][target][interval] = {\n",
    "                                \"far\": far, \n",
    "                                \"frr\": frr,\n",
    "                                \"orig_far\": orig_far,\n",
    "                                \"orig_frr\": orig_frr,\n",
    "                                \"far_change_abs\": orig_far - far,\n",
    "                                \"frr_change_abs\": orig_frr - frr,\n",
    "                                \"total_change_abs\": abs(orig_far - far) + abs(orig_frr - frr),\n",
    "                                \"far_change_rel\": (orig_far / far) * 100,\n",
    "                                \"frr_change_rel\": (orig_frr / frr) * 100\n",
    "                            }\n",
    "                \n",
    "            else:  # Subscenario is not None\n",
    "                result[subscenario] = {}\n",
    "                # Iterate through all other subscenarios\n",
    "                for target_ss in SUB_SETS[scenario] - set([subscenario]):\n",
    "                    result[subscenario][target_ss] = {}\n",
    "                    # Go through all error rates\n",
    "                    for error_rate in error_rates:\n",
    "                        # The following process is identical to the one above, see there for documentation\n",
    "                        threshold = thresholds[scenario][target_ss][error_rate][\"threshold\"]\n",
    "                        orig_far = thresholds[scenario][subscenario][error_rate][\"far\"]\n",
    "                        orig_frr = thresholds[scenario][subscenario][error_rate][\"frr\"]\n",
    "                        \n",
    "                        far, frr = error_rate_for_threshold(rv, threshold)\n",
    "                        result[subscenario][target_ss][error_rate] = {\n",
    "                            \"threshold\": threshold, \n",
    "                            \"far\": far, \n",
    "                            \"frr\": frr,\n",
    "                        }\n",
    "                        if error_rate == \"eer\":\n",
    "                            result[subscenario][target_ss][error_rate][\"orig_far\"] = orig_far\n",
    "                            result[subscenario][target_ss][error_rate][\"orig_frr\"] = orig_frr\n",
    "                            \n",
    "                            if scenario not in robustness_output:\n",
    "                                robustness_output[scenario] = {}\n",
    "                            if subscenario not in robustness_output[scenario]:\n",
    "                                robustness_output[scenario][subscenario] = {}\n",
    "                            if target_ss not in robustness_output[scenario][subscenario]:\n",
    "                                robustness_output[scenario][subscenario][target_ss] = {}\n",
    "                            robustness_output[scenario][subscenario][target_ss][interval] = {\n",
    "                                \"far\": far, \n",
    "                                \"frr\": frr,\n",
    "                                \"orig_far\": orig_far,\n",
    "                                \"orig_frr\": orig_frr,\n",
    "                                \"far_change_abs\": orig_far - far,\n",
    "                                \"frr_change_abs\": orig_frr - frr,\n",
    "                                \"total_change_abs\": abs(orig_far - far) + abs(orig_frr - frr),\n",
    "                                \"far_change_rel\": (orig_far / far) * 100,\n",
    "                                \"frr_change_rel\": (orig_frr / frr) * 100\n",
    "                            }\n",
    "\n",
    "        # Save the results to a JSON file\n",
    "        save_result_json(result, AUDIO_FP, interval, scenario, 'fingerprints_similarity_percent', subscenario=\"robustness\", suffix=\"summary\")\n",
    "\n",
    "        \n",
    "# Okay, now it's time to print out a table of the robustness data\n",
    "# Once again, we go through all pairs of scenarios:\n",
    "for scenario in SCENARIO_SET:\n",
    "    for scenario2 in SCENARIO_SET - set([scenario]):\n",
    "        # Print out table header\n",
    "        # \"scenario\" is the used dataset, scenario2 is the one whose threshold was used\n",
    "        print(\"Robustness\", scenario, scenario2)\n",
    "        # What do these abbreviations mean?\n",
    "        # - Int = Interval that was used\n",
    "        # - FAR and FRR = Obtained false accept / reject rates\n",
    "        # - ofar and ofrr = Original FAR and FRR from the base scenario, for comparison\n",
    "        # - sprd = spread between FAR and FRR, i.e. abs(FAR - FRR)\n",
    "        # - osprd = Spread between original FAR and FRR, i.e. abs(ofar - ofrr)\n",
    "        # - aca and rca = Absolute change in false accept / reject rate, i.e. ofar - far\n",
    "        # - tca = Absolute changes summed up, i.e. aca + rca\n",
    "        # - acr and rcr = Relative changes in FAR and FRR, i.e. (ofar / far) * 100\n",
    "        # - oeer = original EER\n",
    "        # - eer = new EER, i.e. (far + frr) / 2.0\n",
    "        # - eerabs = absolute change in EER\n",
    "        print(\"Int\", \"FAR\", \"FRR\", \"ofar\", \"ofrr\", \"sprd\", \"osprd\", \"aca\", \"rca\", \"tca\", \"acr\", \"rcr\", \"oeer\", \"eer\", \"eerabs\", sep='\\t|')\n",
    "        print(\"-\" + \"-------+\"*14 + \"-------\")\n",
    "        scenpair = robustness_output[scenario][scenario2]\n",
    "        for interval in INTERVALS:\n",
    "            oeer = (scenpair[interval]['orig_far'] + scenpair[interval]['orig_frr']) / 2.0\n",
    "            eer = (scenpair[interval]['far'] + scenpair[interval]['frr']) / 2.0\n",
    "            ospread = abs(scenpair[interval]['orig_far'] - scenpair[interval]['orig_frr'])\n",
    "            spread = abs(scenpair[interval]['far'] - scenpair[interval]['frr'])\n",
    "            print(interval, \n",
    "                  round(scenpair[interval]['far'], 3), \n",
    "                  round(scenpair[interval]['frr'], 3), \n",
    "                  round(scenpair[interval]['orig_far'], 3), \n",
    "                  round(scenpair[interval]['orig_frr'], 3), \n",
    "                  round(spread, 3),\n",
    "                  round(ospread, 3),\n",
    "                  round(scenpair[interval]['far_change_abs'], 3), \n",
    "                  round(scenpair[interval]['frr_change_abs'], 3),\n",
    "                  round(scenpair[interval]['total_change_abs'], 3),\n",
    "                  round(scenpair[interval]['far_change_rel'], 1), \n",
    "                  round(scenpair[interval]['frr_change_rel'], 1),\n",
    "                  round(oeer, 3),\n",
    "                  round(eer, 3),\n",
    "                  round(oeer - eer, 3),\n",
    "                  sep='\\t|')\n",
    "        print(\"\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# Print the same table for all combinations of subscenarios\n",
    "# Process is the same as above.\n",
    "for scenario in SCENARIO_SET:\n",
    "    for subscenario in SUB_LIST[scenario]:\n",
    "        for target_ss in SUB_LIST[scenario]:\n",
    "            if target_ss == subscenario or target_ss is None or subscenario is None:\n",
    "                continue\n",
    "            print(\"Robustness\", scenario, subscenario, target_ss)\n",
    "            print(\"Int\", \"FAR\", \"FRR\", \"ofar\", \"ofrr\", \"sprd\", \"osprd\", \"aca\", \"rca\", \"tca\", \"acr\", \"rcr\", \"oeer\", \"eer\", \"eerabs\", sep='\\t|')\n",
    "            print(\"-\" + \"-------+\"*14 + \"-------\")\n",
    "            scenpair = robustness_output[scenario][subscenario][target_ss]\n",
    "            for interval in INTERVALS:\n",
    "                oeer = (scenpair[interval]['orig_far'] + scenpair[interval]['orig_frr']) / 2.0\n",
    "                eer = (scenpair[interval]['far'] + scenpair[interval]['frr']) / 2.0\n",
    "                ospread = abs(scenpair[interval]['orig_far'] - scenpair[interval]['orig_frr'])\n",
    "                spread = abs(scenpair[interval]['far'] - scenpair[interval]['frr'])\n",
    "                print(interval, \n",
    "                      round(scenpair[interval]['far'], 3), \n",
    "                      round(scenpair[interval]['frr'], 3), \n",
    "                      round(scenpair[interval]['orig_far'], 3), \n",
    "                      round(scenpair[interval]['orig_frr'], 3),\n",
    "                      round(spread, 3),\n",
    "                      round(ospread, 3),\n",
    "                      round(scenpair[interval]['far_change_abs'], 3), \n",
    "                      round(scenpair[interval]['frr_change_abs'], 3),\n",
    "                      round(scenpair[interval]['total_change_abs'], 3),\n",
    "                      round(scenpair[interval]['far_change_rel'], 1), \n",
    "                      round(scenpair[interval]['frr_change_rel'], 1),\n",
    "                      round(oeer, 3),\n",
    "                      round(eer, 3),\n",
    "                      round(oeer - eer, 3),\n",
    "                      sep='\\t|')\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomness\n",
    "To evaluate the randomness of the generated fingerprints, we use techniques developed by Brüsch et al. in their paper [\"On the Secrecy of Publicly Observable Biometric Features: Security Properties of Gait for Mobile Device Pairing\" (CoRR abs/1804.03997)](https://arxiv.org/abs/1804.03997)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on code by Arne Brüsch, cf. https://github.com/abruesch/randomness-figures\n",
    "# It has been adapted to fit our use case. For the details on the original concept, see:\n",
    "# Arne Brüsch, Ngu Nguyen, Dominik Schürmann, Stephan Sigg, and Lars Wolf. 2018. \n",
    "# On the Secrecy of Publicly Observable Biometric Features: Security Properties of Gait for \n",
    "# Mobile Device Pairing. CoRR abs/1804.03997 (2018). https://arxiv.org/abs/1804.03997\n",
    "# The original code is licensed under the GPLv3.\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from scipy.misc import comb\n",
    "from scipy.special import binom\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "def random_walk(key, sum_distribution, transition_count, transition_probs, bitlength=128):\n",
    "    ''' takes string of '0' and '1' and turns them into random walks in a galton board. Effectively computes\n",
    "    the cumulative sums distribution for every prefix of the input string. '''\n",
    "\n",
    "    transition_count[0] += 1\n",
    "    ret = [0]\n",
    "    for i, b in enumerate(key[:bitlength]):\n",
    "        val = -1\n",
    "        if b == '1':\n",
    "            val = 1\n",
    "            transition_probs[i] += 1\n",
    "        ret.append(ret[-1] + val)\n",
    "\n",
    "    sum_distribution.append(ret[-1])\n",
    "    return (sum_distribution, transition_count, transition_probs)\n",
    "\n",
    "\n",
    "def markov_transitions(transition_probs, transition_count, bits=128):\n",
    "    ''' transition probabilities from every nth to (n+1)th bit. '''\n",
    "    norm_transition_probs = [transition_probs[x] / transition_count[0]\n",
    "                             for x in range(0, bits)]\n",
    "    plt.clf()\n",
    "    plt.xlabel('nth bit')\n",
    "    plt.ylabel('Probability for 1')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlim([0, bits])\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.plot(norm_transition_probs, color='b')\n",
    "    plt.plot([0.0, bits], [0.5, 0.5], 'k:')\n",
    "    plt.show()\n",
    "    print(\"Markov:\", np.median(norm_transition_probs))\n",
    "\n",
    "from scipy.misc import comb\n",
    "\n",
    "def binomial_plot(n=256):\n",
    "    ''' Theoretical binomial distribution that is plotted as red line into figure.'''\n",
    "    x = np.arange(n)\n",
    "    # y = list(map(lambda xi: comb(n,xi)/2**n, x))\n",
    "    y = list(binom(n,x)*0.5**x*(0.5)**(n-x))\n",
    "    x = list(map(lambda r: r-n/2, x))\n",
    "    plt.plot(x,y,color='r')\n",
    "\n",
    "#def binomial_plot(bins,n):\n",
    "#    variance =  n * 0.5 * 0.5\n",
    "#    sigma = np.sqrt(variance)\n",
    "#    y = mlab.normpdf(np.asarray(list(range(-128,128))), 0, sigma)\n",
    "#    plt.plot(np.asarray(list(range(-128,128))),y,color='r')\n",
    "\n",
    "def distribution(sum_distribution, bits=128, dist_xlim=None, save_to=None):\n",
    "    ''' Plots the cumulative sums distribution and saves figure. '''\n",
    "    plt.clf()\n",
    "    if dist_xlim is not None:\n",
    "        plt.xlim(dist_xlim)\n",
    "    else:\n",
    "        plt.xlim([-bits, bits])\n",
    "    count, bins, ignored = plt.hist(sum_distribution, color='#007a9b', range=(-bits,bits), normed=True,rwidth=0.5, bins=bits)\n",
    "    binomial_plot(bits*2)\n",
    "    # Get the current axes\n",
    "    ax = plt.gca()\n",
    "    # Get limits\n",
    "    start, end = ax.get_xlim()\n",
    "    if abs(start) > 150:\n",
    "        # We are appearantly plotting a very long fingerprint, let's make the ticks more sparse\n",
    "        ax.xaxis.set_ticks(np.arange(start, end+1, 100))\n",
    "        \n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    \n",
    "    if save_to is not None:\n",
    "        plt.savefig(save_to, format='eps', dpi=1000)\n",
    "    plt.show()\n",
    "    print(\"Median of distribution:\", np.median(sum_distribution))\n",
    "    \n",
    "\n",
    "\n",
    "def plot_rand_walk(keys, bits=128, dist_xlim=None, save_distribution_to=None):\n",
    "    ''' turns string consisting of '0' and '1' into random walks. While the walks are currently not plotted, the distribution\n",
    "    of the cumulative sums along with the markov transitions are computed and plotted using them.'''\n",
    "    sum_distribution = []\n",
    "    transition_count = [0]\n",
    "    transition_probs = {x: 0 for x in range(0, bits)}\n",
    "\n",
    "    matplotlib.rcParams['axes.prop_cycle'] = cycler('color',\n",
    "                                                    ['#e78a33', '#eda766', '#8d4959', '#aa7782', '#bdcd61', '#cdda89',\n",
    "                                                     '#8a9c33', '#a7b566'])\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "    plt.ylabel('Sum')\n",
    "    plt.xlabel('Keylength')\n",
    "\n",
    "    plt.ylim([-bits, bits])\n",
    "    for key in keys:\n",
    "        sum_distribution, transition_count, transition_probs = random_walk(key, sum_distribution, transition_count, transition_probs, bits)\n",
    "    plt.tight_layout()\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    distribution(sum_distribution, bits, dist_xlim=dist_xlim, save_to=save_distribution_to)\n",
    "    markov_transitions(transition_probs, transition_count, bits)\n",
    "\n",
    "\n",
    "def plot_heat_map(keys, pt=plt):\n",
    "    ''' creates heatmap of random walks.'''\n",
    "    intensity = 1\n",
    "    heatmap_array = []\n",
    "    for key in keys:\n",
    "        val = int(key[0])\n",
    "        for i, bit in enumerate(key[1:128]):\n",
    "            if bit == '1':\n",
    "                val = val + 1\n",
    "            elif bit == '0':\n",
    "                val = val - 1\n",
    "            heatmap_array.append([i, val])\n",
    "\n",
    "    np_srt_heat = np.asarray(heatmap_array)\n",
    "    X = np.take(np_srt_heat, [0], axis=1).flatten()\n",
    "    Y = np.take(np_srt_heat, [1], axis=1).flatten()\n",
    "    # bins = (range(max(X)), range(min(Y),max(Y)))\n",
    "    bins = (range(128), range(min(Y), max(Y)))\n",
    "    H, xedges, yedges = np.histogram2d(X, Y, bins=bins, normed=False)\n",
    "    # H = H.T\n",
    "    # H = H\n",
    "    H = intensity * H\n",
    "    # 'viridis'\n",
    "    pt.xlabel('Sum')\n",
    "    pt.ylabel('Keylength')\n",
    "    pt.imshow(H, cmap='viridis', norm=matplotlib.colors.LogNorm(), interpolation='nearest', origin='upper',\n",
    "              extent=[yedges[0], yedges[-1], xedges[-1], xedges[0]])\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    # Plot triangle\n",
    "    plus_y = [i for i in range(-1, 127)]\n",
    "    plus_x = [i for i in range(1, 129)]\n",
    "    minus_y = [126 - i for i in range(0, 128)]\n",
    "    minus_x = [-127 + i for i in range(1, 129)]\n",
    "    plt.tight_layout()\n",
    "    pt.plot(plus_x, plus_y, color='r')\n",
    "    pt.plot(minus_x, minus_y, color='r')\n",
    "\n",
    "\n",
    "def apply_plot_heat_map(keys):\n",
    "    ''' Calls plot_heat_map() and additionally saves the plotted figure.'''\n",
    "    plot_heat_map(keys)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a couple functions to deal with fingerprints. As this was ported from a different piece of code, the constants are slightly different and need to be redefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/media/seemoo/data/zia-data/results/\"\n",
    "PREFIX_JSON='/home/seemoo/plots/json'\n",
    "\n",
    "CAR_EXP = 'CarExp/'\n",
    "OFF_EXP = 'OfficeExp/'\n",
    "MOBILE_EXP = 'MobileExp/'\n",
    "# Sensor mapping: car experiment\n",
    "SENSORS_CAR1 = ['01', '02', '03', '04', '05', '06']\n",
    "SENSORS_CAR2 = ['07', '08', '09', '10', '11', '12']\n",
    "SENSORS_CAR = SENSORS_CAR1 + SENSORS_CAR2\n",
    "\n",
    "# Sensor mapping: office experiment\n",
    "SENSORS_OFFICE1 = ['01', '02', '03', '04', '05', '06', '07', '08']\n",
    "SENSORS_OFFICE2 = ['09', '10', '11', '12', '13', '14', '15', '16']\n",
    "SENSORS_OFFICE3 = ['17', '18', '19', '20', '21', '22', '23', '24']\n",
    "SENSORS_OFFICE = SENSORS_OFFICE1 + SENSORS_OFFICE2 + SENSORS_OFFICE3\n",
    "\n",
    "SENSORS_MOBILE = [\"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\"]\n",
    "\n",
    "INT_5s = '5sec/'\n",
    "INT_10s = '10sec/'\n",
    "INT_15s = '15sec/'\n",
    "INT_30s = '30sec/'\n",
    "INT_1min = '1min/'\n",
    "INT_2min = '2min/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the actual function to deal with fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(sensor1, sensor2, feature, jv, flip='01'):\n",
    "    if feature == 'audioFingerprint':\n",
    "        if sensor2 == flip:\n",
    "            return jv['results'][sensor1]['fingerprint_chunk1']\n",
    "        else:\n",
    "            return jv['results'][sensor1]['fingerprint_chunk2']\n",
    "    if feature == 'noiseFingerprint':\n",
    "        if type(jv['results'][sensor1]['fingerprint_noise_lev1']) is str:\n",
    "            if sensor2 == flip:\n",
    "                return jv['results'][sensor1]['fingerprint_noise_lev1']\n",
    "            else:\n",
    "                return jv['results'][sensor1]['fingerprint_noise_lev2']\n",
    "        else:\n",
    "            if sensor2 == flip:\n",
    "                return ''.join([jv['results'][sensor1]['fingerprint_noise_lev1'][idx] for idx in sorted(jv['results'][sensor1]['fingerprint_noise_lev1'].keys())])\n",
    "            else:\n",
    "                return ''.join([jv['results'][sensor1]['fingerprint_noise_lev2'][idx] for idx in sorted(jv['results'][sensor1]['fingerprint_noise_lev2'].keys())])\n",
    "    if feature == 'lux_miettinen':\n",
    "        res = ''\n",
    "        i = 128\n",
    "        while i < len(jv[\"results\"].keys()):\n",
    "            x = sorted(jv[\"results\"].keys())[i-1]\n",
    "            res += jv[\"results\"][x]\n",
    "            i += 128\n",
    "        i = min(i, len(jv[\"results\"].keys()))\n",
    "        x = sorted(jv[\"results\"].keys())[i-1]\n",
    "        res += jv[\"results\"][x]\n",
    "        return res\n",
    "\n",
    "def import_fps(sensors, scenario, feature, interval):\n",
    "    \"\"\"Load results from a specific file and return them as python dict.\"\"\"\n",
    "    rv = {}\n",
    "    if scenario == CAR_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-01/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "            for sen in sensors:\n",
    "                if sen == '01':\n",
    "                    path = pattern.format('02')\n",
    "                else:\n",
    "                    path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = []\n",
    "                    for res in j['results']:\n",
    "                        rv[sen].append(get_val(res, sen, feature, j))\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = [get_val(None, sen, feature, j)]\n",
    "    elif scenario == MOBILE_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-02/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "            for sen in sensors:\n",
    "                if sen == '02':\n",
    "                    path = pattern.format('03')\n",
    "                else:\n",
    "                    path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = []\n",
    "                    for res in j['results']:\n",
    "                        rv[sen].append(get_val(res, sen, feature, j, flip=\"02\"))\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = [get_val(None, sen, feature, j, flip=\"02\")]\n",
    "    if scenario == OFF_EXP:\n",
    "        if feature is not 'lux_miettinen':\n",
    "            for day in ['1_0-24h', '2_24-48h', '3_48-72h', '4_72-96h','5_96-120h', '6_120-144h', '7_144-168h']:\n",
    "                pattern = BASE_DIR + scenario + 'audio/{}/Sensor-01/audio/' + feature + \"/\" + interval + 'Sensor-{}.json.gz'\n",
    "                for sen in sensors:\n",
    "                    if sen == '01':\n",
    "                        path = pattern.format(day, '02')\n",
    "                    else:\n",
    "                        path = pattern.format(day, sen)\n",
    "                    with gzip.open(path, 'rt') as fo:\n",
    "                        j = json.loads(fo.read())\n",
    "                        if sen not in rv:\n",
    "                            rv[sen] = []\n",
    "                        for res in j['results']:\n",
    "                            rv[sen].append(get_val(res, sen, feature, j))\n",
    "        else:\n",
    "            pattern = BASE_DIR + scenario + 'Sensor-{}/lux/lux_miettinen/' + interval + 'delta_abs-10.0/delta_rel-0.1/fp_len-128/result.json.gz'\n",
    "            for sen in sensors:\n",
    "                path = pattern.format(sen)\n",
    "                with gzip.open(path, 'rt') as fo:\n",
    "                    j = json.loads(fo.read())\n",
    "                    rv[sen] = [get_val(None, sen, feature, j)]\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the functions defined above to evaluate and plot the bit distributions for the schemes in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario, sensors in [(CAR_EXP, SENSORS_CAR), (OFF_EXP, SENSORS_OFFICE), (MOBILE_EXP, SENSORS_MOBILE)]:\n",
    "    for interval in reversed([INT_5s, INT_10s, INT_15s, INT_30s, INT_1min, INT_2min]):\n",
    "        a = import_fps(sensors, scenario, 'audioFingerprint', interval)\n",
    "        for i in a.keys():\n",
    "            keys = a[i]\n",
    "            print(scenario, interval, i)\n",
    "            plot_rand_walk(keys, bits=496, dist_xlim=[-200, 200], save_distribution_to='/home/seemoo/plots/img/%saudioFingerprint/audio/sensor-%s-%s-full.eps' % (scenario, i, interval[:-1]))\n",
    "\n",
    "            keys = []\n",
    "            for key in a[i]:\n",
    "                for k in range(16):\n",
    "                    tkey = key[k*31:k*31+31]\n",
    "                    keys.append(tkey)\n",
    "            plot_rand_walk(keys, bits=31, save_distribution_to='/home/seemoo/plots/img/%saudioFingerprint/audio/sensor-%s-%s-sub.eps' % (scenario, i, interval[:-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
